W99-0621	W95-0107	o	We do not consider mixed features between words and POS tags as in (l:tamshaw and Marcus, 1995), that is, a single feature consists of either words or tags.
D07-1006	J97-3002	o	In designing LEAF, we were also inspired by dependency-based alignment models (Wu, 1997; Alshawi et al. , 2000; Yamada and Knight, 2001; Cherry and Lin, 2003; Zhang and Gildea, 2004).
N03-1028	W02-1001	o	On the machine-learning side, it would be interesting to generalize the ideas of large-margin classi cation to sequence models, strengthening the results of Collins (2002) and leading to new optimal training algorithms with stronger guarantees against over tting.
W03-0315	J93-2003	o	In our approach, equation (1) is further normalized so that the probability for different lengths of F is comparable at the word level: m m j n i ijm eft l EFP /1 10 )|( )1( 1 )|(       + =  == (2) The alignment models described in (Brown et al. , 1993) are all based on the notion that an alignment aligns each source word to exactly one target word.
D09-1079	P03-1021	o	We held out 300 sentences for minimum error rate training (MERT) (Och, 2003) and optimised the parameters of the feature functions of the decoder for each experimental run.
H05-1057	J93-2003	p	The IBM models have shown good performance in machine translation, and especially so within certain families of languages, for example in translating between French and English or between Sinhalese and Tamil (Brown et al. , 1993; Weerasinghe, 2004).
I08-4024	W96-0213	o	2 Maximum Entropy In this bakeoff, our basic model is based on the framework described in the work of Ratnaparkhi (1996) which was applied for English POS tagging.
N06-1033	J97-3002	o	This problem can be cast as an instance of synchronous ITG parsing (Wu, 1997).
J01-3001	P95-1026	o	Yarowsky (1995) dealt with this problem largely by producing an unsupervised learning algorithm that generates probabilistic decision list models of word senses from seed collocates.
W09-2310	P02-1040	o	The highest BLEU score (Papineni et al., 2002) was chosen as the optimization criterion.
P04-1047	P04-1041	o	We utilise the automatic annotation algorithm of (Cahill et al. , 2004b) to derive a version of Penn-II where each node in each tree is annotated with an LFG functional annotation (i.e. an attribute value structure equation).
N09-1021	D07-1091	o	In particular, we adopt the approach of phrase-based statistical machine translation (Koehn et al., 2003; Koehn and Hoang, 2007).
J07-3002	P06-1097	o	F-Measure with an appropriate setting of  will be useful during the development process of new alignment models, or as a maximization criterion for discriminative training of alignment models (Cherry and Lin 2003; Ayan, Dorr, and Monz 2005; Ittycheriah and Roukos 2005; Liu, Liu, and Lin 2005; Fraser and Marcu 2006; Lacoste-Julien et al. 2006; Moore, Yih, and Bode 2006).
H05-1028	J92-4003	o	More specifically, we use a class-based bigram model from (Brown et al, 1992): )|()|()|( 11  = iiiiii ccPcwPwwP (3) In Equation (3), c i is the class of the word w i, which could be a syntactic class or a semantic class.
C96-2205	P90-1034	o	2.3 Measuring the similarity between classes (step 3) In step 3, we measure the similarity between two primitive classes by using the method given by Hindle (Hindle, 1990).
P09-1054	P07-1104	p	There is usually not a considerable difference between the two methods in terms of the accuracy of the resulting model (Gao et al., 2007), but L1 regularization has a significant advantage in practice.
W02-1011	J96-1002	p	5.2 Maximum Entropy Maximum entropy classiflcation (MaxEnt, or ME, for short) is an alternative technique which has proven efiective in a number of natural language processing applications (Berger et al. , 1996).
W02-0908	J92-4003	o	These tasks include collocation discovery (Pearce, 2001), smoothing and model estimation (Brown et al. , 1992; Clark and Weir, 2001) and text classi cation (Baker and McCallum, 1998).
I08-1060	J96-1002	o	There are other types of variations for phrases; for example, insertion, deletion or substitution of words, and permutation of words such as view point and point of view are such variations (Daille et al., 1996).
D07-1115	P02-1053	p	(Turney, 2002) is one of the most famous work that discussed learning polarity from corpus.
W04-2415	W02-1001	o	The learning algorithm follows the global strategy introduced in (Collins, 2002) and adapted in (Carreras and M`arquez, 2004b) for partial parsing tasks.
P06-2034	W02-1001	o	In comparison with shallow semantic analysis tasks, such as wordsense disambiguation (Ide and Jeaneronis, 1998) and semantic role labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates, semantic parsing (Kate et al. , 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005) pursues a more ambitious goal  mapping natural language sentences to complete formal meaning representations (MRs), where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and so on.
W06-3601	P03-1021	o	Feature weights of both systems are tuned on the same data set.3 For Pharaoh, we use the standard minimum error-rate training (Och, 2003); and for our system, since there are only two independent features (as we always fix  = 1), we use a simple grid-based line-optimization along the language-model weight axis.
P05-1033	N03-1017	o	When we run a phrase-based system, Pharaoh (Koehn et al. , 2003; Koehn, 2004a), on this sentence (using the experimental setup described below), we get the following phrases with translations: (4) [Aozhou] [shi] [yu] [Bei Han] [you] [bangjiao]1 [de shaoshu guojia zhiyi] [Australia] [is] [dipl.
P07-1092	N03-1017	o	The translation models and lexical scores were estimated on the training corpus whichwasautomaticallyalignedusingGiza++(Och et al. , 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al. , 2003).
W06-1403	N03-1003	o	Our experience suggests that disjunctive LFs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases (Barzilay and Lee, 2003).
C08-1029	P07-1010	o	One way of obtaining a suitable granularity of nodes is to introduce latent classes, such as the Semi-Markov class model (Okanohara and Tsujii, 2007).
W00-1313	J90-1003	o	The association relationship between two words can be indicated by their mutual information, which can be further used to discover phrases \[Church :& Hanks (1990)\].
N07-1008	J93-2003	o	1.2 Statistical modeling for translation Earlier work in statistical machine translation (Brown et al. , 1993) is based on the noisy-channel formulation where T = arg max T p(TjS) = argmax T p(T)p(SjT) (1) where the target language model p(T) is further decomposed as p(T) / productdisplay i p(tijti1, . . ., tik+1) where k is the order of the language model and the translation model p(SjT) has been modeled by a sequence of five models with increasing complexity (Brown et al. , 1993).
P06-2065	J93-2003	o	This is similar to Model 3 of (Brown et al. , 1993), but without null-generated elements or re-ordering.
J07-4004	W96-0213	o	Related Work The first application of log-linear models to parsing is the work of Ratnaparkhi and colleagues (Ratnaparkhi, Roukos, and Ward 1994; Ratnaparkhi 1996, 1999).
W00-0733	W95-0107	o	Chunks can be represented with bracket structures but alternatively one can use a tagging representation which classifies words as being inside a chunk (I), outside a chunk (O) or at a chunk boundary (B) (Ramshaw and Marcus, 1995).
I05-2039	P02-1040	o	Because it is not feasible here to have humans judge the quality of many sets of translated data, we rely on an array of well known automatic evaluation measures to estimate translation quality :  BLEU (Papineni et al. 2002) is the geometric mean of the n-gram precisions in the output with respect to a set of reference translations.
D09-1008	C08-1041	o	Due to the lack of a good Arabic parser compatible with the Sakhr tokenization that we used on the source side, we did not test the source dependency LM for Arabic-to-English MT. When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al., 2008).
J04-3004	P95-1026	p	The Yarowsky (1995) algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics.
W06-3111	J97-3002	n	An alternative method (Wu, 1997) makes decisions at the end but has a high computational requirement.
W96-0213	J93-2004	o	Previous uses of this model include language modeling(Lau et al. , 1993), machine translation(Berger et al. , 1996), prepositional phrase attachment(Ratnaparkhi et al. , 1994), and word morphology(Della Pietra et al. , 1995).
E09-1061	P07-1019	n	Huang and Chiang (2007) give an informal example, but do not elaborate on it.
P98-1092	J93-1007	o	However morphosyntactic features alone cannot verify the terminological status of the units extracted since they can also select non terms (see Smadja 1993).
C08-1117	C98-2122	p	Among these measures, the most important are Wu & Palmers (Wu and Palmer, 1994), Resniks (Resnik, 1995) and Lins (Lin, 1998).
W09-2412	D07-1007	o	Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems will not be required to translate the whole context but just the target word.
D09-1028	C98-2122	o	Curran (2002) and Lin (1998) use syntactic features in the vector definition.
W09-1804	J93-2003	o	For now, we consider it to be one where:  Every foreign word is aligned exactly once (Brown et al., 1993).
I08-2110	N03-1003	o	Recently, some work has been done on corpusbased paraphrase extraction (Lin and Pantel, 2001; Barzilay and Lee, 2003).
J03-1004	J96-2004	o	As Carletta (1996) notes, many tasks in computational linguistics are simply more difficult than the content analysis classifications addressed by Krippendorff, and according to Fleiss (1981), kappa values between .4 and .75 indicate fair to good agreement anyhow.
A94-1006	J90-1003	o	In particular, mutual information (Church and Hanks, 1990; Wu and Su, 1993) and other statistical methods such as (Smadja, 1993) and frequency-based methods such as (Justeson and Katz, 1993) exclude infrequent phrases because they tend to introduce too much noise.
E09-1090	N06-1020	o	A totally different approach to improving the accuracy of our parser is to use the idea of selftraining described in (McClosky et al., 2006).
J98-4002	P90-1034	o	Predicate argument structures, which consist of complements (case filler nouns and case markers) and verbs, have also been used in the task of noun classification (Hindle 1990).
P98-2182	J93-1003	o	For the final ranking, we chose the log likelihood statistic outlined in Dunning (1993), which is based upon the co-occurrence counts of all nouns (see Dunning for details).
D09-1071	D07-1031	o	Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater & Griffiths (2007) leave this question as future work.
P07-1028	P90-1034	o	We will be using the similarity metrics shown in Table 1: Cosine, the Dice and Jaccard coefficients, and Hindles (1990) and Lins (1998) mutual information-based metrics.
P06-2063	P02-1053	o	Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Semantic orientation classification is a task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005).
E06-2018	P95-1026	p	Although the relative success of previous disambiguation systems (e.g. Yarowsky, 1995) suggests that this should be the case, the effect has usually not been quantified as the emphasis was on a task-based evaluation.
W08-0603	N07-1015	o	A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser.
P01-1051	J96-2004	o	Analyze resulting findings to determine a progression of competence In (Michaud et al. , 2001) we discuss the initial steps we took in this process, including the development of a list of error codes documented by a coding manual, the verification of our manual and coding scheme by testing inter-coder reliability in a subset of the corpus (where we achieved a Kappa agreement score (Carletta, 1996) of a0 a1a3a2a5a4a7a6 )2, and the subsequent tagging of the entire corpus.
P99-1020	P95-1026	o	WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky, 1995) (Resnik, 1997).
N04-4026	J93-2003	o	The orientation model is related to the distortion model in (Brown et al. , 1993), but we do not compute a block alignment during training.
P07-1029	W95-0107	o	NP chunks in the shared task data are BaseNPs, which are non-recursive NPs, a definition first proposed by Ramshaw and Marcus (1995).
W00-0714	J96-1002	o	We have used the Improved Iterative Scaling algorithm (IIS) (Berger et al. , 1996).
W04-0412	J93-1007	p	Many efficient techniques exist to extract multiword expressions, collocations, lexical units and idioms (Church and Hanks, 1989; Smadja, 1993; Dias et al. , 2000; Dias, 2003).
D09-1115	P07-1040	p	In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.
W97-0205	J93-1007	o	MI is defined in general as follows: y) I ix y) = log2 P(x) P(y) We can use this definition to derive an estimate of the connectedness between words, in terms of collocations (Smadja, 1993), but also in terms of phrases and grammatical relations (Hindle, 1990).
W00-0731	W96-0213	o	2 The Tagger We used Ratnaparkhi's maximum entropybased POS tagger (Ratnaparkhi, 1996).
W04-1806	J93-1003	o	Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993), or require human-annotated training data with relation information for each domain (Craven et al. 1998).
N01-1023	A92-1018	p	(Cutting et al. , 1992) reported very high results (96% on the Brown corpus) for unsupervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes.
I08-1012	D07-1013	o	F 1 =2precisionrecall/(precision + recall) the figure, we find that F 1 score decreases when dependency length increases as (McDonald and Nivre, 2007) found.
P09-1057	P08-1085	o	Some previous approaches (Toutanova and Johnson, 2008; Goldberg et al., 2008) handle unknown words explicitly using ambiguity class components conditioned on various morphological features, and this has shown to produce good tagging results, especially when dealing with incomplete dictionaries.
C02-1008	J93-2003	p	Another kind of popular approaches to dealing with query translation based on corpus-based techniques uses a parallel corpus containing aligned sentences whose translation pairs are corresponding to each other (Brown et al. , 1993; Dagan et al. , 1993; Smadja et al. , 1996).
W05-0404	J93-2004	o	In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, a29a30, given the words, a31, is output (Weischedel et al. , 1993): a29 a30 a20a22a32a34a33a36a35a38a37a39a32a41a40 a42 a43a45a44 a30a47a46 a31a49a48a17a20a22a32a34a33a50a35a38a37a39a32a41a40 a42 a43a45a44 a31 a46a30 a48 a43a51a44 a30 a48 Since we do not have enough data which is manually tagged with part-of-speech tags for our applications, we used Penn Treebank (Marcus et al. , 1994) as our training set.
H05-1021	N03-1017	o	Phrase-pairs are then extracted from the word alignments (Koehn et al. , 2003).
W02-1039	J93-2003	o	The first work in SMT, done at IBM (Brown et al. , 1993), developed a noisy-channel model, factoring the translation process into two portions: the translation model and the language model.
C08-1104	P04-1035	o	2 Related Work There has been extensive research in opinion mining at the document level, for example on product and movie reviews (Pang et al., 2002; Pang and Lee, 2004; Dave et al., 2003; Popescu and Etzioni, 2005).
W07-1217	J93-2004	o	Empirical evaluation has been done with the ERG on a small set of texts from the Wall Street Journal Section 22 of the Penn Treebank (Marcus et al. , 1993).
W08-1130	J96-1002	o	These feature functions fi were used to train a maximum entropy classifier (Berger et al., 1996) (Le, 2004)thatassignsaprobabilitytoaREregiven context cx as follows: p(re| cx) = Z(cx)exp nsummationdisplay i=1 ifi(cx,re) where Z(cx) is a normalizing sum and the i are the parameters (feature weights) learned.
P06-2034	W02-1001	o	The model is composed of three parts (Collins, 2002a): a set of candidate SAPTs GEN, which is the top n SAPTs of a sentence from SCISSOR; a function  that maps a sentence Inputs: A set of training examples (xi,yi ), i = 1n, where xi is a sentence, and yi is a candidate SAPT that has the highest similarity score with the gold-standard SAPT Initialization: Set W = 0 Algorithm: For t = 1T,i = 1n Calculate yi = argmaxyGEN(xi) (xi,y)  W If (yi negationslash= yi ) then W = W +(xi,yi )  (xi,yi) Output: The parameter vector W Figure 2: The perceptron training algorithm.
D09-1003	J96-1002	p	To estimate the parameters of the MEMM+pred model we turn to the successful Maximum Entropy (Berger et al., 1996) parameter estimation method.
P09-1020	P03-1021	o	GIZA++ (Och and Ney, 2003) and the heuristics grow-diag-final-and are used to generate m-ton word alignments.
P07-2056	A92-1018	p	Stochastic models (Cutting et al. , 1992; Dermatas et al. , 1995; Brants, 2000) have been widely used in POS tagging for simplicity and language independence of the models.
P06-1021	W96-0213	o	This test set was tagged using MXPOST (Ratnaparkhi, 1996) which was itself trained on Switchboard.
P02-1024	J92-4003	o	Recent research [Yamamoto et al. , 2001] shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models, which use the same clusters for both words [Brown et al. , 1992].
A00-2026	J96-1002	o	The features used in NLG2 are described in the next section, and the feature weights aj, obtained from the Improved Iterative Scaling algorithm (Berger et al. , 1996), are set to maximize the likelihood of the training data.
J97-3002	J93-2003	o	Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong.
P09-1024	W04-1013	o	There has been a sizable amount of research on structure induction ranging fromlinearsegmentation(Hearst, 1994)tocontent modeling (Barzilay and Lee, 2004).
J03-1002	J93-2003	o	An analysis of the alignments shows that smoothing the fertility probabilities significantly reduces the frequently occurring problem of rare words forming garbage collectors in that they tend to align with too many words in the other language (Brown, Della Pietra, Della Pietra, Goldsmith, et al. 1993).
E06-1038	W02-1001	p	Averaging has been shown to reduce overfitting (Collins, 2002) as well as reliance on the order of the examples during training.
W05-0709	J96-1002	o	The principle of maximum entropy states that when one searches among probability distributions that model the observed data (evidence), the preferred one is the one that maximizes the entropy (a measure of the uncertainty of the model) (Berger et al. , 1996).
J99-1003	J93-2003	o	A limitation of Church's method, and therefore also of Dagan, Church, and Gale's method, is that orthographic cognates exist only among languages with similar alphabets (Church et al. 1993).
P06-1067	J93-2003	o	Distortion models were first proposed by (Brown et al. , 1993) in the so-called IBM Models.
P97-1009	J93-1003	o	The likelihood ratio is obtained by treating word and Ic as a bigram and computed with the formula in (Dunning, 1993).
J04-4002	P03-1021	o	The alignment a J 1 that has the highest probability (under a certain model) is also called the Viterbi alignment (of that model): a J 1 = argmax a J 1 p   (f J 1, a J 1 | e I 1 ) (8) A detailed comparison of the quality of these Viterbi alignments for various statistical alignment models compared to human-made word alignments can be found in Och and Ney (2003).
W05-0712	J93-2003	n	A word based approach depends upon traditional statistical machine translation techniques such as IBM Model1 (Brown et al. , 1993) and may not always yield satisfactory results due to its inability to handle difficult many-to-many phrase translations.
C96-1083	P90-1034	o	Section 4 compares our results to Itindle's ones (Hindle, 1990).
P05-1063	W02-1001	o	For this paper, we train the parameter vector  using the perceptron algorithm (Collins, 2004; Collins, 2002).
N03-2017	J97-3002	o	Methods such as (Wu, 1997), (Alshawi et al. , 2000) and (Lopez et al. , 2002) employ a synchronous parsing procedure to constrain a statistical alignment.
P08-1116	N03-1003	o	For example, Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences.
C08-1082	J05-4002	o	The linear kernel derived from the L1 distance is the same as the difference-weighted token-based similarity measure of Weeds and Weir (2005).
W03-0501	P97-1003	o	2 Previous Work Other researchers have investigated the topic of automatic generation of abstracts, but the focus has been different, e.g., sentence extraction (Edmundson, 1969; Johnson et al, 1993; Kupiec et al. , 1995; Mann et al. , 1992; Teufel and Moens, 1997; Zechner, 1995), processing of structured templates (Paice and Jones, 1993), sentence compression (Hori et al. , 2002; Knight and Marcu, 2001; Grefenstette, 1998, Luhn, 1958), and generation of abstracts from multiple sources (Radev and McKeown, 1998).
W97-0108	P95-1026	p	Unsupervised algorit~m~ such as (Yarowsky, 1995) have reported good accuracy that rivals that of supervised algorithms.
W08-0329	P02-1040	o	The translation quality is measured by three MT evaluation metrics: TER (Snover et al., 2006), BLEU (Papineni et al., 2002), and METEOR (Lavie and Agarwal, 2007).
D07-1080	P03-1021	o	The hierarchical phrase translation pairs are extracted in a standard way (Chiang, 2005): First, the bilingual data are word alignment annotated by running GIZA++ (Och and Ney, 2003) in two directions.
W06-3601	N03-1017	n	2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al. , 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order.
W09-0407	P02-1040	o	For the WMT 2009 Workshop, we selected a linear combination of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as optimization criterion,  := argmax{(2BLEU)TER}, based on previous experience (Mauser et al., 2008).
P03-1008	P95-1026	o	All features encountered in the training data are ranked in the DL (best evidence first) according to the following loglikelihood ratio (Yarowsky, 1995): Log Pr(reading i jfeature k ) P j6=i Pr(reading j jfeature k ) We estimated probabilities via maximum likelihood, adopting a simple smoothing method (Martinez and Agirre, 2000): 0.1 is added to both the denominator and numerator.
D09-1022	D07-1007	o	Second, instead of disambiguating phrase senses as in (Carpuat and Wu, 2007), we model word selection independently of the phrases used in the MT models.
P08-1064	P02-1040	o	The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002).
C04-1067	J96-1002	o	In the above equation, P(ti) and P(wi;t) are estimated by the maximum-likelihood method, and the probability of a POC tag ti, given a character wi (P(tijwi;ti 2 TPOC)) is estimated using ME models (Berger et al. , 1996).
N09-2059	C98-2122	o	The thesaurus was produced using the metric described by Lin (1998) with input from the grammatical relation data extracted using the 90 million words of written English from the British National Corpus (BNC) (Leech, 1992) using the RASP parser (Briscoe and Carroll, 2002).
W09-2415	P08-1052	o	The patterns will be manually constructed following the approach of Hearst (1992) and Nakov and Hearst (2008).6 The example collection for each relation R will be passed to two independent annotators.
W01-1403	P97-1003	o	We automatically converted the phrase structure output of the Collins parser into the syntactic dependency representation used by our syntactic realizer, RealPro (Lavoie and Rambow, 1997).
W08-0333	J93-2003	o	203 Estimating the parameters for these models is more difficult (and more computationally expensive) than with the models considered in the previous section: rather than simply being able to count the word pairs and alignment relationships and estimate the models directly, we must use an existing model to compute the expected counts for all possible alignments, and then use these counts to update the new model.7 This training strategy is referred to as expectationmaximization (EM) and is guaranteed to always improve the quality of the prior model at each iteration (Brown et al., 1993; Dempster et al., 1977).
A92-1013	J90-1003	o	In (Calzolari and Bindi, 1990), (Church and Hanks, 1990) the significance of an association (x,y) is measured by the mutual information I(x,y), i.e. the probability of observing x and y together, compared with the probability of observing x and y independently.
P01-1046	J92-4003	o	(1999) and Lee (1999)) can be generally divided into three types: discounting (Katz, 1987), class-based smoothing (Resnik, 1993; Brown et al. , 1992; Pereira et al. , 1993), and distance-weighted averaging (Grishman and Sterling, 1994; Dagan et al. , 1999).
D09-1073	J97-3002	o	In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG (Bracketing Transduction Grammar) constraints (Wu, 1997).
W04-2708	J93-2004	n	Since Czech is a language with relatively high degree of word-order freedom, and its sentences contain certain syntactic phenomena, such as discontinuous constituents (non-projective constructions), which cannot be straightforwardly handled using the annotation scheme of Penn Treebank (Marcus et al. , 1993; Linguistic Data Consortium, 1999), based on phrase-structure trees, we decided to adopt for the PCEDT the dependency-based annotation scheme of the Prague Dependency Treebank  PDT (Linguistic Data Consortium, 2001).
W05-0814	J93-2003	o	We solve this using the local search defined in (Brown et al. , 1993).
J99-4003	J93-2004	o	"This source is very important for repairs that do not have initial retracing, and is the mainstay of the ""parser-first"" approach (e.g. , 550 Heeman and Allen Modeling Speakers' Utterances Dowding et al. 1993)--keep trying alternative corrections until one of them parses."
P09-1058	P08-1101	o	0.9595 0.9590 0.9611 0.9085 0.9134 0.9152 Table 8: Comparison of F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0.
D08-1024	P06-1096	n	Sentence-level approximations to B exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform B computations in the context of a setOof previously-translated sentences, following Watanabe et al.
W06-2405	J93-1003	o	For each candidate triple, the log-likelihood (Dunning, 1993) and salience (Kilgarriff and Tugwell, 2001) scores were calculated.
P09-1101	J96-2004	o	(Carletta 1996) is another method of comparing inter-annotator agreement 0 30 60 90 120 150 1 2 3 4 5 6 7 8 9 10 11 >11 120 25 10 32 3 4 3 1 2 0 17 2 Nu mb er of an not ators Number of dialogues completed Figure 2.
N06-1013	J93-2003	n	stance, the IBM models (Brown et al. , 1993) can be improved by adding more context dependencies into the translation model using a ME framework rather than using only p(f j |e i ) (Garcia-Varea et al. , 2002).
A94-1009	J93-2004	p	Preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although a semi-automatic approach can be used (Marcus et al. , 1993), it is a good thing to reduce the human involvement as much as possible.
C02-1125	J93-1003	p	For instance, the mutual information (Church et al. 1990) and log-likelihood ratio (Dunning 1993; Cohen 1995) have been widely used for extracting word bigrams.
W04-2312	J96-2004	n	The class-based kappa statistic of (Cohen, 1960; Carletta, 1996) cannot be applied here, as the classes vary depending on the number of ambiguities per entry in the lexicon.
P06-2069	J90-1003	o	One can also examine the distribution of character or word ngrams, e.g. Language Modeling (Croft and Lafferty, 2003), phrases (Church and Hanks, 1990; Lewis, 1992), and so on.
W96-0103	J92-4003	o	2 Hierarchical Clustering of Words Several algorithms have been proposed for automatically clustering words based on a large corpus (Jardino and Adda 91, Brown et al. 1992, Kneser and Ney 1993, Martin et al. 1995, Ueberla 1995).
W07-1505	J93-2004	o	The Dublin Core Metadata Initiative3 established a de facto standard for the Semantic Web.4 For (computational) linguistics proper, syntactic annotation schemes, such as the one from the Penn Treebank (Marcus et al. , 1993), or semantic annotations, such as the one underlying ACE (Doddington et al. , 2004), are increasingly being used in a quasi standard way.
J03-1005	J93-2003	o	For placing the head the center function center(i) (Brown et al. [1993] uses the notation circledot i ) is used: the average position of the source words with which the target word e i1 is aligned.
D09-1009	N06-1041	o	For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate.
W02-1018	J93-2003	o	Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c. Sentence pair (S2, T2) offers strong evidence that b c in language S means the same thing as x in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that a in language S means the same thing as y in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word  as it is the case in the IBM models (Brown et al. , 1993)  it is impossible to learn that the phrase b c in language S means the same thing as word x in language T. The IBM Model 4 (Brown et al. , 1993), for example, converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a.2 Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure 2To train the IBM-4 model, we used Giza (Al-Onaizan et al. , 1999).
W06-0707	W04-1013	o	Additionally, automatic evaluation of content coverage using ROUGE (Lin, 2004) was explored in 2004.
W02-0817	J93-2004	o	One of the first large scale hand tagging efforts is reported in (Miller et al. , 1993), where a subset of the Brown corpus was tagged with WordNet July 2002, pp.
C00-1058	J93-1003	o	We adopted log-likelihood ratio (Danning 1993), which gave the best pertbrmance among crude non-iterative methods in our test experiments 6 .
I08-1069	N06-1041	p	Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly.
N09-2032	P07-1080	o	5.1 The statistical parser The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance.
I05-2027	W04-1013	o	ROUGE-LCS calculated the longest common 2 Details of our official DUC 2004 headline generation system can be found in Doran et al.
D07-1080	P02-1040	o	The algorithm is slightly different from other online training algorithms (Tillmann and Zhang, 2006; Liang et al. , 2006) in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, i.e. BLEU (Papineni et al. , 2002).
W96-0205	A92-1018	o	Generalized Forward Backward Reestimation Generalization of the Forward and Viterbi Algorithm In English part of speech taggers, the maximization of Equation (1) to get the most likely tag sequence, is accomplished by the Viterbi algorithm (Church, 1988), and the maximum likelihood estimates of the parameters of Equation (2) are obtained from untagged corpus by the ForwardBackward algorithm (Cutting et al. , 1992).
P05-1069	N03-1017	o	Two block sets are derived for each of the training sets using a phrase-pair selection algorithm similar to (Koehn et al. , 2003; Tillmann and Xia, 2003).
C08-1114	J06-3003	o	 PairClass generates probability estimates, whereas Turney (2006) uses a cosine measure of similarity.
W05-0826	J93-2003	o	Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by (Brown et al. , 1993).
N09-1019	D07-1074	p	Much later work (Evans, 2003; Etzioni et al., 2005; Cucerzan, 2007; Pasca, 2004) relies on the use of extremely large corpora which allow very precise, but sparse features.
D09-1161	J93-2004	o	Averaged Perceptron Algorithm 5 Experiments We evaluate our method on both Chinese and English syntactic parsing task with the standard division on Chinese Penn Treebank Version 5.0 and WSJ English Treebank 3.0 (Marcus et al. 1993) as shown in Table 1.
W01-1408	J93-2003	o	Many statistical translation models (Brown et al. , 1993; Vogel et al. , 1996; Och and Ney, 2000b) try to model word-to-word correspondences between source and target words.
P05-1009	P02-1040	o	We evaluate accuracy performance using two automatic metrics: an identity metric, ID, which measures the percent of sentences recreated exactly, and BLEU (Papineni et al. , 2002), which gives the geometric average of the number of uni-, bi-, tri-, and four-grams recreated exactly.
P06-2089	J93-2004	o	4 Experiments We evaluated our classifier-based best-first parser on the Wall Street Journal corpus of the Penn Treebank (Marcus et al. , 1993) using the standard split: sections 2-21 were used for training, section 22 was used for development and tuning of parameters and features, and section 23 was used for testing.
N06-1058	W05-0909	o	Examples of such knowledge sources include stemming and TF-IDF weighting (Babych and Hartley, 2004; Banerjee and Lavie, 2005).
D07-1126	W02-1001	o	Averaging has been shown to help reduce overfitting (McDonald et al. , 2005a; Collins, 2002).
J05-1003	W02-1001	o	See Collins (2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron algorithm.
J05-3002	P02-1040	n	This restriction is necessary because the problem of optimizing many-to-many alignments 5 Our preliminary experiments with n-gram-based overlap measures, such as BLEU (Papineni et al. 2002) and ROUGE (Lin and Hovy 2003), show that these metrics do not correlate with human judgments on the fusion task, when tested against two reference outputs.
P06-1071	J96-1002	o	This leads to a good amount of work in this area (Ratnaparkhi et al. , 1994; Berger et al. , 1996; Pietra et al, 1997; Zhou et al. , 2003; Riezler and Vasserman, 2004) In the most basic approach, such as Ratnaparkhi et al.
W97-0202	J93-2004	o	1 Introduction This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus (Marcus et al. 1993).
D09-1134	N06-1041	o	Prototype-drive learning (Haghighi and Klein, 2006) specifies prior knowledge by providing a few prototypes (i.e., canonical example words) for each label.
C00-2121	J92-4003	o	Methods that use bigrams (Brown et al. , 1992) or trigrams (Martin et al. , 1998) cluster words considering as a word's context the one or two immediately adjacent words and employ as clustering criteria the minimal loss of average 836 nmtual information and the perplexity improvement respectively.
W05-0310	J93-2004	o	The latter group did an experiment early on in which they found that manual tagging took about twice as long as correcting [automated tagging], with about twice the interannotator disagreement rate and an error rate that was about 50% higher (Marcus et al. 1993).
P09-1059	J93-2004	o	Penn Treebank (Marcus et al., 1993) the HPSG LinGo Redwoods Treebank (Oepen et al., 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006).
W09-0201	J06-3003	o	We solve SAT analogies with a simplified version of the method of Turney (2006).
P06-2071	P95-1026	p	(Yarowsky, 1995) demonstrated that semi-supervised WSD could be successful.
W07-0704	W05-0909	o	Note that using stems and their synonyms as used in METEOR (Banerjee and Lavie, 2005) could also be considered for word similarity.
P05-1033	N03-1017	o	To do this, we first identify initial phrase pairs using the same criterion as previous systems (Och and Ney, 2004; Koehn et al. , 2003): Definition 1.
W03-0401	P97-1003	p	2.1 Lexicalized parse trees The first successful work on syntactic disambiguation was based on lexicalized probabilistic context-free grammar (LPCFG) (Collins, 1997; Charniak, 1997).
C08-1014	N03-1017	o	Our MT baseline system is based on Moses decoder (Koehn et al., 2007) with word alignment obtained from GIZA++ (Och et al., 2003).
W07-1202	W02-1001	p	We chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks; in particular, Collins (2002) reported good performance for a perceptron tagger compared to a Maximum Entropy tagger.
W07-0403	J97-3002	o	This ITG constraint is characterized by the two forbidden structures shown in Figure 1 (Wu, 1997).
P01-1050	J93-2003	o	a65 The rest of the factors denote distorsion probabilities (d), which capture the probability that words change their position when translated from one language into another; the probability of some French words being generated from an invisible English NULL element (pa6 ), etc. See (Brown et al. , 1993) or (Germann et al. , 2001) for a detailed discussion of this translation model and a description of its parameters.
P99-1051	J93-1003	o	Levin (1993) assumes that the syntactic realization of a verb's arguments is directly correlated with its meaning (cf.
D08-1071	W95-0107	o	Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions.
P06-2117	J93-2003	o	2 Statistical Word Alignment Model According to the IBM models (Brown et al. , 1993), the statistical word alignment model can be generally represented as in equation (1).
W07-2218	P07-1080	o	The second approximation proposed in (Titov and Henderson, 2007) takes into consideration the fact that, after each decision is made, all the preceding latent variables should have their means i updated.
P01-1027	J93-2003	o	If we assign a probability a15a17a16a19a18 a12 a13a7a21a20a4a6a5a7a23a22 to each pair of strings a18 a12a14a13a7a25a24 a4 a5a7 a22, then according to Bayes decision rule, we have to choose the target string that maximizes the product of the target language model a15a17a16a19a18 a12a14a13a7 a22 and the string translation model a15a17a16a19a18a26a4a6a5 a7 a20 a12 a13 a7 a22 . Many existing systems for statistical machine translation (Berger et al. , 1994; Wang and Waibel, 1997; Tillmann et al. , 1997; Nieen et al. , 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al. , 1993): The correspondence between the words in the source and the target string is described by alignments that assign one target word position to each source word position.
P09-1114	N07-1015	o	Following our previous work (Jiang and Zhai, 2007b), we extract features from a sequence representation and a parse tree representation of each relation instance.
N06-1040	P97-1003	o	Probability estimates of the RHS given the LHS are often smoothed by making a Markov assumption regarding the conditional independence of a category on those more than k categories away (Collins, 1997; Charniak, 2000): P(X  Y1Yn)= P(Y1|X) nY i=2 P(Yi|X,Y1 Yi1)  P(Y1|X) nY i=2 P(Yi|X,Yik Yi1).
P02-1043	W96-0213	o	We therefore ran the dependency model on a test corpus tagged with the POS-tagger of Ratnaparkhi (1996), which is trained on the original Penn Treebank (see HWDep (+ tagger) in Table 3).
W09-2416	P08-1052	o	The SemEval-2010 task we present here builds on thework ofNakov (Nakovand Hearst, 2006; Nakov, 2007; Nakov, 2008b), where NCs are paraphrased by combinations of verbs and prepositions.
I08-2116	H05-1087	o	The training methods of LRM-F and SVM-F were useful to improve the F M -scores of LRM and SVM, as reported in (Jansche, 2005; Joachims, 2005).
J01-2004	P97-1003	o	Examples of this are bilexical grammars--such as Eisner and Satta (1999), Charniak (1997), Collins (1997)--where the lexical heads of each constituent are annotated on both the rightand left-hand sides of the context-free rules, under the constraint that every constituent inherits the lexical head from exactly one of its children, and the lexical head of a POS is its terminal item.
D07-1070	J04-3004	o	Drawing on Abneys (2004) analysis of the Yarowsky algorithm, we perform bootstrapping by entropy regularization: we maximize a linear combination of conditional likelihood on labeled data and confidence (negative Renyi entropy) on unlabeled data.
W97-0127	J92-4003	o	"The concept of mutual information, taken from information theory, was proposed as a measure of word association (Church 1990; ""Jelinek et al. 1990,1992; Dagan, 1995;)."
D09-1021	P02-1040	o	For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah.
C00-2105	W95-0107	p	Ramshaw and Marcus (Ramshaw and Marcus, 1995) successflflly applied Eric Brill's transformation-based learning method to the chunking problem.
W02-1504	J93-2004	o	consistency among raters who may have different levels of fluency in the source language, raters are not shown the original French or Spanish sentence (for similar methodologies, see Ringger et al. , 2001; White et al. , 1993).
D09-1054	P08-1081	o	We made use of the same data set as introduced in (Cong et al., 2008; Ding et al., 2008).
P09-1087	N03-1017	p	1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years (Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), and often outperform phrase-based systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy.
W06-1643	W04-1013	o	Empirical evaluations using two standard summarization metricsthe Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%.
P06-1091	P02-1040	o	We show translation results in terms of the automatic BLEU evaluation metric (Papineni et al. , 2002) on the MT03 Arabic-English DARPA evaluation test set consisting of a212a89a212a89a87 sentences with a98a89a212a161a213a89a214a89a215 Arabic words with a95 reference translations.
P09-2086	P08-1058	o	By using 8-bit floating point quantization 1 , N-gram language models are compressed into 10 GB, which is comparable to a lossy representation (Talbot and Brants, 2008).
W07-0701	P03-1021	o	Model weights were trained separately for all 3 systems using minimum error rate training to maximize BLEU (Och, 2003) on the development set (dev).
W97-1004	J93-1007	o	Now with the availability of large-scale corpus, automatic acquisition of word compositions, especially word collocations from them have been extensively studied(e.g. , Choueka et al. 1988; Church and Hanks 1989; Smadja 1993).
W02-2001	J93-2004	o	Any linguistic annotation required during the extraction process, therefore, is produced through automatic means, and it is only for reasons of accessibility and comparability with other research that we choose to work over the Wall Street Journal section of the Penn Treebank (Marcus et al. , 1993).
P98-2184	J93-2004	o	This can be done automatically with unparsed corpora (Briscoe and Carroll 1997, Manning 1993, Ushioda et al. 1993), from parsed corpora such as Marcus et al.'s (1993) Treebank (Merlo 1994, Framis 1994) or manually as was done for COMLEX (Macleod and Grishman 1994).
E99-1013	J90-1003	o	Mutual information compares the probability of the co-occurence of words a and b with the independent probabilities of occurrence of a and b (Church and Hanks, 1990).
P99-1027	J93-2003	o	This model is trained on approximately 5 million sentence pairs of Hansard (Canadian parliamentary) and UN proceedings which have been aligned on a sentence-by-sentence basis by the methods of (Brown et al. , 1991), and then further aligned on a word-by-word basis by methods similar to (Brown et al. , 1993).
D08-1014	W06-1642	o	Much of the work in subjectivity analysis has been applied to English data, though work on other languages is growing: e.g., Japanese data are used in (Kobayashi et al., 2004; Suzuki et al., 2006; Takamura et al., 2006; Kanayama and Nasukawa, 2006), Chinese data are used in (Hu et al., 2005), and German data are used in (Kim and Hovy, 2006).
P07-1089	N03-1017	o	We compared our system Lynx against a freely available phrase-based decoder Pharaoh (Koehn et al. , 2003).
W08-2101	P07-1080	p	3 The Syntactic and Semantic Parser Architecture To achieve the complex task of joint syntactic and semantic parsing, we extend a current state-of-theart statistical parser (Titov and Henderson, 2007) to learn semantic role annotation as well as syntactic structure.
W07-2072	P02-1053	o	Following Hatzivassiloglou and McKeown (1997) and Turney (2002), we decided to observe how often the words from the headline co-occur with each one of the six emotions.
W09-0431	P03-1021	o	We use GIZA++ (Och and Ney, 2003) for  5  176 word alignment, and the Pharaoh system suite to build the phrase table and decode (Koehn, 2004).
P09-1026	N07-1039	o	A number of works in product review mining (Hu and Liu, 2004; Popescu et al., 2005; Kobayashi et al., 2005; Bloom et al., 2007) automatically find features of the reviewed products.
W05-0408	P02-1053	o	c2005 Association for Computational Linguistics Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms Michael Gamon Anthony Aue Natural Language Processing Group Natural Language Processing Group Microsoft Research Microsoft Research mgamon@microsoft.com anthaue@microsoft.com Abstract We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in Turney (2002) and Turney and Littman (2002).
J98-2002	J93-1007	o	As we remarked earlier, however, the input data required by our method (triples) could be generated automatically from unparsed corpora making use of existing heuristic rules (Brent 1993; Smadja 1993), although for the experiments we report here we used a parsed corpus.
P09-1028	P07-1056	o	Amazon Reviews: The dataset contains product reviews taken from Amazon.com from 4 product types: Kitchen, Books, DVDs, and Electronics (Blitzer et al., 2007).
H05-1057	J93-2003	o	Further details are in the original paper (Brown et al. , 1993).
P06-1097	P03-1021	p	Och (2003) has described an ef cient exact one-dimensional error minimization technique for a similar search problem in machine translation.
J04-4002	P03-1021	o	Looking at the results of the recent machine translation evaluations, this approach seems currently to give the best results, and an increasing number of researchers are working on different methods for learning phrase translation lexica for machine translation purposes (Marcu and Wong 2002; Venugopal, Vogel, and Waibel 2003; Tillmann 2003; Koehn, Och, and Marcu 2003).
C04-1114	P02-1040	o	Both calculate the precision of a translation by comparing it to a reference translation and incorporating a length penalty (Doddington, 2001; Papineni et al. , 2002).]
P07-1091	N03-1017	o	The translation table is obtained as described in (Koehn et al. , 2003), i.e. the alignment tool GIZA++ is run over the training data in both translation directions, and the two alignTest Setting BLEU B1 standard phrase-based SMT 29.22 B2 (B1) + clause splitting 29.13 Table 2: Experiment Baseline Test Setting BLEU BLEU 2-ary 2,3-ary 1 rule 29.77 30.31 2 ME (phrase label) 29.93 30.49 3 ME (left,right) 30.10 30.53 4 ME ((3)+head) 30.24 30.71 5 ME ((3)+phrase label) 30.12 30.30 6 ME ((4)+context) 30.24 30.76 Table 3: Tests on Various Reordering Models The 3rd column comprises the BLEU scores obtained by reordering binary nodes only, the 4th column the scores by reordering both binary and 3-ary nodes.
C08-3006	W05-0909	o	This is con rmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) where scores range between 0 (worst) and 1 (best).
I08-2116	H05-1087	o	C, A, and B are computed for training dataset D as C = summationtext M m=1 y (m) y (m) , A = summationtext M m=1 y (m) , and B = summationtext M m=1 y (m) . In (Jansche, 2005), y (m) was approximated by using the discriminative and logistic functions shown in Eqs.
H05-1034	W02-1001	o	3.1 Definition The following set-up, adapted from Collins (2002), was used for all three discriminative training methods: 266  Training data is a set of input-output pairs.
P05-1012	P04-1015	o	Collins and Roark (2004) presented a linear parsing model trained with an averaged perceptron algorithm.
N06-2033	J93-2004	o	Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank (Marcus et al. , 1993).
N04-1041	P03-1001	o	We compared our system with the concepts in WordNet and Fleischman et al.s instance/concept relations (Fleischman et al. 2003).
W09-2303	J97-3002	o	Related work includes Wu (1997), Zens and Ney (2003) and Wellington et al.
W08-0617	W02-1002	o	It is believed that improvement can be achieved by training the generative model based on a discriminative optimization criteria (Klein and Manning, 2002) in which the training procedure is designed to maximize the conditional probability of the parses given the sentences in the training corpus.
E09-1089	W04-1013	o	We use only the words that are content words (nouns, verbs, or adjectives) and not in the stopword list used in ROUGE (Lin, 2004).
W06-3101	W05-0909	o	An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 proposed in (Banerjee and Lavie, 2005).
W09-0439	P03-1021	p	Although they obtained consistent and stable performance gains for MT, these were inferior to the gains yielded by Ochs procedure in (Och, 2003).
P98-2230	J97-3002	o	The model employs a stochastic version of an inversion transduction grammar or ITG (Wu, 1995c; Wu, 1995d; Wu, 1997).
P08-1064	J97-3002	p	Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003).
P07-1080	J93-2004	o	We used the Penn Treebank WSJ corpus (Marcus et al. , 1993) to perform the empirical evaluation of the considered approaches.
W07-1602	J93-2004	o	For this reason, each preposition and verb was assigned a weight based on the proportion of occurrences of that word in the Penn Treebank (Marcus et al. , 1993) which are labelled with a spatial meaning.
W06-1613	J96-2004	o	4Following Carletta (1996), we measure agreement in Kappa, which follows the formula K = P(A)P(E)1P(E) where P(A) is observed, and P(E) expected agreement.
C08-2026	J97-3002	n	Inside-out alignments (Wu, 1997), such as the one in Example 1.3, cannot be induced by any of these theories; in fact, there seems to be no useful synchronous grammar formalisms available that handle inside-out alignments, with the possible exceptions of synchronous tree-adjoining grammars (Shieber and Schabes, 1990), Bertsch and Nederhof (2001) and generalized multitext grammars (Melamed et al., 2004), which are all way more complex than ITG, STSG and (2,2)-BRCG.
P02-1062	W02-1001	o	(Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper.
P03-1064	W95-0107	o	We repeat Ramshaw and Marcus Transformation Based NP chunking (Ramshaw and Marcus, 1995) algorithm by substituting supertags for POS tags in the dataset.
P07-1104	N04-1039	o	L1 or Lasso regularization of linear models, introduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g. , Goodman 2003; Riezler and Vasserman 2004).
P05-1033	N03-1017	o	For our experiments we used the following features, analogous to Pharaohs default feature set:  P( | ) and P( | ), the latter of which is not found in the noisy-channel model, but has been previously found to be a helpful feature (Och and Ney, 2002);  the lexical weights Pw( | ) and Pw( | ) (Koehn et al. , 2003), which estimate how well the words in  translate the words in ;2  a phrase penalty exp(1), which allows the model to learn a preference for longer or shorter derivations, analogous to Koehns phrase penalty (Koehn, 2003).
W06-1609	J93-2003	o	This feature, which is based on the lexical parameters of the IBM Model 1 (Brown et al. , 1993), provides a complementary probability for each tuple in the translation table.
W06-0123	W96-0213	o	)|(maxarg* STPT T = (1) Then we assume that the tagging of one character is independent of each other, and modify formula 1 as  == = = = n i ii tttT nn tttT ctP ccctttPT n n 1 2121 * )|(maxarg )|(maxarg 21 21 (2) Beam search (n=3) (Ratnaparkhi,1996) is applied for tag sequence searching, but we only search the valid sequences to ensure the validity of searching result.
D09-1017	P08-1036	o	Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006).
W03-1807	J93-1007	o	In his Xtract system, Smadja (1993) first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance, strength and spread, and then examined concordances of the bi-grams to find longer frequent multiword units.
W05-1007	P04-1048	p	This paper demonstrates several of the characteristics and benefits of SemFrame (Green et al. , 2004; Green and Dorr, 2004), a system that produces such a resource.
C96-2157	P95-1026	o	An alternative method we considered was to estimate certain conditional probabilities, similarly to the formula used in (Yarowsky, 1995): SW(t) log P(p C A/t) f(t, A)f(A) = ~ log (2) P(p C R/t) f(t, .R)f(.l~) Here f(A) is (an estimate of) the probability that any given candidate phrase will be accepted by the spotter, and f(R) is the probability that this phrase is rejected, i.e., f(R) = l-f (A).
D09-1063	P02-1053	o	Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation (Turney, 2002; Wilson et al., 2005; Pang and Lee, 2008).
P07-1079	J93-2004	o	We created a dependency training corpus based on the Penn Treebank (Marcus et al. , 1993), or more specifically on the HPSG Treebank generated from the Penn Treebank (see section 2.2).
P03-1064	W96-0213	o	4.4 Related Work (Chen, 2001) implemented an MEMM model for supertagging which is analogous to the POS tagging model of (Ratnaparkhi, 1996).
C04-1141	J93-1003	o	Almost all of these measures can be grouped into one of the following three categories: a0 frequency-based measures (e.g. , based on absolute and relative co-occurrence frequencies) a0 information-theoretic measures (e.g. , mutual information, entropy) a0 statistical measures (e.g. , chi-square, t-test, log-likelihood, Dices coefficient) The corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties (Dunning, 1993; Manning and Schutze, 1999) and their suitability for the task of collocation extraction (see Evert and Krenn (2001) and Krenn and Evert (2001) for recent evaluations).
J93-1001	J92-1002	o	Model Bits / Character ASCII Huffman code each char Lempel-Ziv (Unix TM compress) Unigram (Huffman code each word) Trigram Human Performance 8 5 4.43 2.1 (Brown, personal communication) 1.76 (Brown et al. 1992) 1.25 (Shannon 1951) The cross entropy, H, of a code and a source is given by: H(source, code) = ~ ~ Pr(s, h I source) log 2 Pr(s I h, code) s h where Pr(s, h I source) is the joint probability of a symbol s following a history h given the source.
W08-0402	P07-1019	o	However, with the algorithms proposed in (Huang and Chiang, 2005; Chiang, 2007; Huang and Chiang, 2007), it is possible to develop a general-purpose decoder that can be used by all the parsing-based systems.
P98-2216	J93-1007	o	It seems nevertheless that all 2Church and Hanks (1989), Smadja (1993) use statistics in their algorithms to extract collocations from texts.
W03-0419	W95-0107	o	This tagging scheme is the IOB scheme originally put forward by Ramshaw and Marcus (1995).
J07-1003	J93-2003	o	Therefore, we determine the maximal translation probability of the target word e over the source sentence words: p ibm1 (e|f J 1 ) = max j=0,,J p(e|f j ) (18) where f 0 is the empty source word (Brown et al. 1993).
D07-1003	W06-3104	p	2 Related Work To model the syntactic transformation process, researchers in these fieldsespecially in machine translationhave developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al. , 2005; Galley et al. , 2006; Smith and Eisner, 2006, inter alia).
W03-0416	J92-4003	o	The second type has clear interpretation as a probability model, but no criteria to determine the number of clusters (Brown et al. , 1992; Kneser and Ney, 1993).
P06-1134	P02-1053	o	The third exploits automatic subjectivity analysis in applications such as review classification (e.g. , (Turney, 2002; Pang and Lee, 2004)), mining texts for product reviews (e.g. , (Yi et al. , 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005)), summarization (e.g. , (Kim and Hovy, 2004)), information extraction (e.g. , (Riloff et al. , 2005)), 1Note that sentiment, the focus of much recent work in the area, is a type of subjectivity, specifically involving positive or negative opinion, emotion, or evaluation.
P08-1076	D07-1083	o	As our approach for incorporating unlabeled data, we basically follow the idea proposed in (Suzuki et al., 2007).
W08-0403	J97-3002	o	2 Related Work Syntax-based translation models engaged with SCFG have been actively investigated in the literature (Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Galley et al., 2004; Satta and Peserico, 2005).
N03-1028	W02-1001	n	The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron.
N09-1005	P08-1045	o	There are many techniques for transliteration and back-transliteration, and they vary along a number of dimensions:  phoneme substitution vs. character substitution  heuristic vs. generative vs. discriminative models  manual vs. automatic knowledge acquisition We explore the third dimension, where we see several techniques in use:  Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008).
D09-1058	J92-4003	o	First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm (Brown et al., 1992).
E09-3005	P07-1033	o	Therefore, whenever we have access to a large amount of labeled data from some source (out-of-domain), but we would like a model that performs well on some new target domain (Gildea, 2001; Daume III, 2007), we face the problem of domain adaptation.
W09-0409	P07-1040	p	The availability of the TER software has made it easy to build a high performance system combination baseline (Rosti et al., 2007).
P09-1041	D07-1014	o	This weak supervision has been encoded using priors and initializations (Klein and Manning, 2004; Smith, 2006), specialized models (Klein and Manning, 2004; Seginer, 2007; Bod, 2006), and implicit negative evidence (Smith, 2006).
P09-1058	P08-1102	o	In this paper, we used CTB 5.0 (LDC2005T01) as our main corpus, defined the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b), and designed our experiments to explore the impact of the training corpus size on our approach.
W99-0704	J93-2004	o	The WSJNPVP set consists of part-of speech tagged Wall Street Journal material (Marcus, Santorini & Marcinkiewicz, 1993), supplemented with syntactic tags indicating noun phrase and verb phrase boundaries (Daelemans et al, 1999iii).
W06-1643	W04-1013	o	To find these pairs automatically, wetrainedanon-sequentiallog-linearmodel that achieves a .902 accuracy (Galley et al. , 2004).
P03-1013	J93-2004	o	The annotation scheme (Skut et al. , 1997) is modeled to a certain extent on that of the Penn Treebank (Marcus et al. , 1993), with crucial differences.
W07-1502	J93-2004	p	After the success in syntactic (Penn TreeBank (Marcus et al. , 1993)) and propositional encodings (Penn PropBank (Palmer et al. , 2005)), more sophisticated semantic data (such as temporal (Pustejovsky et al. , 2003) or opinion annotations (Wiebe et al. , 2005)) and discourse data (e.g. , for anaphora resolution (van Deemter and Kibble, 2000) and rhetorical parsing (Carlson et al. , 2003)) are being generated.
D07-1070	J04-3004	o	In his analysis of Yarowsky (1995), Abney (2004) formulates several variants of bootstrapping.
J94-4005	J90-1003	o	Lexical collocation functions, especially those determined statistically, have recently attracted considerable attention in computational linguistics (Calzolari and Bindi 1990; Church and Hanks 1990; Sekine et al. 1992; Hindle and Rooth 1993) mainly, though not exclusively, for use in disambiguation.
W01-1208	P95-1026	o	Since word senses are often associated with domains (Yarowsky, 1995), word senses can be consequently distinguished by way of determining the domain of each description.
H05-1023	J93-2003	o	Word alignments traditionally are based on IBM Models 1-5 (Brown et al. , 1993) or on HMMs (Vogel et al. , 1996).
C08-1040	C04-1162	o	Our method is based on the ones described in (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007), The objective of this paper is to dynamically rank speakers or participants in a discussion.
W07-0413	J96-1002	o	The probability distributions of these binary classifiers are learnt using maximum entropy model (Berger et al. , 1996; Haffner, 2006).
I08-2087	J97-3002	o	However, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (Wu 1997; Chiang 2005).
W09-1606	P04-1035	o	3.3 Language Model (LM) As a second baseline we use the classification based on the language model using overlapping ngram sequences (n was set to 8) as suggested by Pang & Lee (2004, 2005) for the English language.
W05-0810	J93-1003	o	First, we considered single sentences as documents, and tokens as sentences (we define a token as a sequence of characters delimited by 1In our case, the score we seek to globally maximize by dynamic programming is not only taking into account the length criteria described in (Gale and Church, 1993) but also a cognate-based one similar to (Simard et al. , 1992).
D09-1056	D07-1074	o	Some researchers (Cucerzan, 2007; Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.
P98-2251	J93-2004	o	Charniak (Charniak et al. , 1993) gives a thorough explanation of the equations for an HMM model, and Kupiec (Kupiec, 1992) describes an HMM tagging system in detail.
D09-1073	P02-1040	o	Besides the the case-sensitive BLEU-4 (Papineni et al., 2002) used in the two experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation.
P09-2037	J93-2003	p	This is an important feature from the MT viewpoint, since the decomposition into translation model and language model proved to be extremely useful in statistical MT since (Brown et al., 1993).
C00-2123	J93-2003	o	The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al. , 1993; Ney et al. , 2000).
D08-1018	P97-1003	o	Therefore in Collins (1997) grammar rules are already factorized into a set of probabilities.
C00-2130	P99-1048	o	Our system attempts to recognize these syntactic patterus; in addition, it considers as unfamiliar some definites occurring in 4This list was developed by hand; more recently, Bean and Riloff (1999) proposed methods for autolnatically extracting fl'om a corpus such special predicates, i.e., heads that correlate well with discourse novelty.
W97-1005	J93-2004	o	Both data were extracted from the Penn Treebank Wall Street Journal (WSJ) Corpus (Marcus et al. , 1993).
D08-1093	N04-1035	o	As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008).
P95-1054	J93-1003	o	It forms a baseline for performance evaluations, but is prone to sparse data problems (Dunning, 1993).
D07-1102	J93-2004	o	This task evaluated parsing performance on 10 languages: Arabic, Basque, Catalan, Chinese, Czech, English, Greek, Hungarian, Italian, and Turkish using data originating from a wide variety of dependency treebanks, and transformations of constituency-based treebanks (Hajic et al. , 2004; Aduriz et al. , 2003; Mart et al. , 2007; Chen et al. , 2003; Bohmova et al. , 2003; Marcus et al. , 1993; Johansson and Nugues, 2007; Prokopidis et al. , 2005; Csendes et al. , 2005; Montemagni et al. , 2003; Oflazer et al. , 2003).
P07-1006	P95-1026	o	2 Related Work WSD approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge, manually coded or extracted from lexical resources (Agirre and Rigau, 1996; Lesk 1986); (b) corpus-based approaches, which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models (Yarowsky, 1995; Schtze 1998); and (c) hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge (Ng and Lee 1996; Stevenson and Wilks, 2001).
W07-1110	J96-1002	o	5.2 Maximum Entropy Model We use the Maximum Entropy (ME) Model (Berger et al. , 1996) for our classification task.
W08-0333	N03-1017	o	4 Phrase-Based Translation In phrase-based translation, the translation process is modeled by splitting the source sentence into phrases (a contiguous string of words) and translating the phrases as a unit (Och et al., 1999; Koehn et al., 2003).
P06-1033	P97-1003	n	This is well illustrated by the Collins parser (Collins, 1997; Collins, 1999), scrutinized by Bikel (2004), where several transformations are applied in order to improve the analysis of noun phrases, coordination and punctuation.
N09-1027	P02-1040	o	Feature weights vector are trained discriminatively in concert with the language model weight to maximize the BLEU (Papineni et al., 2002) automatic evaluation metric via Minimum Error Rate Training (MERT) (Och, 2003).
J03-4003	P97-1003	o	Previous workthe generative models described in Collins (1996) and the earlier version of these models described in Collins (1997)conditioned on punctuation as surface features of the string, treating it quite differently from lexical items.
J02-2003	J93-1003	o	8 An alternative formula for G 2 is given in Dunning (1993), but the two are equivalent.
C08-1008	D07-1031	o	Standard sequence prediction models are highly effective for supertagging, including Hidden Markov Models (Bangalore and Joshi, 1999; Nielsen, 2002), Maximum Entropy Markov Models (Clark, 2002; Hockenmaier et al., 2004; Clark and Curran, 2007), and Conditional Random Fields (Blunsom and Baldwin, 2006).
N03-1019	J93-2003	n	The ATTM attempts to overcome the deficiencies of word-to-word translation models (Brown et al. , 1993) through the use of phrasal translations.
P09-1041	D07-1014	o	This generates tens of millions features, so we prune those features that occur fewer than 10 total times, as in (Smith and Eisner, 2007).
D09-1123	P07-1019	o	To circumvent these computational limitations, various pruning techniques are usually needed, e.g., (Huang and Chiang, 2007).
P97-1066	P90-1034	o	Statistical or probabilistic methods are often used to extract semantic clusters from corpora in order to build lexical resources for ANLP tools (Hindle, 1990), (Zernik, 1990), (Resnik, 1993), or for automatic thesaurus generation (Grefenstette, 1994).
C00-2094	P95-1026	o	The SENSEVAL '~tan(lard is clearly beaten by the earlier results of Yarowsky (1995) (96.5 % precision) and Schiitze (1992) (92 % precision).
W99-0621	W95-0107	o	The results are comparable to other results reported using the Inside/Outside method (Ramshaw and Marcus, 1995) (see Table 7.
C08-1008	D07-1031	o	FollowingJohnson(2007),Iusevariational Bayes EM (Beal, 2003) during the M-step for the transition distribution: l+1j|i = f(E[ni,j] +i)f(E[n i] + |C|i) (3) f(v) = exp((v)) (4) 60 (v) = braceleftBigg g(v 1 2) ifv> 7 (v+ 1)  1v o.w.
I08-1067	P03-1021	o	The weights for the various components of the model (phrase translation model, language model, distortion model etc.) are set by minimum error rate training (Och, 2003).
P07-1001	P06-1097	o	It has been shown that human knowledge, in the form of a small amount of manually annotated parallel data to be used to seed or guide model training, can significantly improve word alignment F-measure and translation performance (Ittycheriah and Roukos, 2005; Fraser and Marcu, 2006).
W09-2307	N03-1017	o	2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (Koehn et al., 2003).
N07-1072	J96-2004	o	The metric we used is the kappa statistic (Carletta, 1996), which factors out the agreement that is expected by chance: )(1 )()( EP EPAP   = where P(A) is the observed agreement among the raters, and P(E) is the expected agreement, i.e., the probability that the raters agree by chance.
D09-1117	J93-2003	o	Their experiments were performed using a decoder based on IBM Model 4 using the translation techniques developed at IBM (Brown et al., 1993).
E09-1060	W96-0213	o	Thus, we used the five taggers, MBL (Daelemans et al., 1996), MXPOST (Ratnaparkhi, 1996), fnTBL (Ngai and Florian, 2001), TnT, and IceTagger3, in the same manner as described in (Loftsson, 2006), but with the following minor changes.
D09-1050	J93-2003	o	However, since we are interested in the word counts that correlate to w, we adopt the concept of the translation model proposed by Brown et al (1993).
D07-1033	P05-1045	n	Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al. , 2005; Roth and Yih, 2005; Krishnan and Manning, 2006; Nakagawa and Matsumoto, 2006), these present a problem that the types of non-local features are somewhat constrained.
P07-1057	P90-1034	o	Hindle (1990) uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns.
J02-2003	J90-1003	o	Equation (10) is of interest because the ratio p(C | v, r)/p(C | r) can be interpreted as a measure of association between the verb v and class C. This ratio is similar to pointwise mutual information (Church and Hanks 1990) and also forms part of Resniks association score, which will be introduced in Section 6.
N09-1051	J92-4003	o	4.2 Models with Prior Distributions Minimum discrimination information models (Della Pietra et al., 1992) are exponential models with a prior distribution q(y|x): p(y|x) = q(y|x)exp( summationtextF i=1 ifi(x,y)) Z(x) (14) The central issue in performance prediction for MDI models is whether q(y|x) needs to be accounted for.
P02-1044	P95-1026	o	This implementation is exactly the one proposed in (Yarowsky 1995), and we will denote it as MB-D hereafter.
P05-1022	P97-1003	o	The Collins parser (Collins, 1997) does use dynamic programming in its search.
N03-1032	J90-1003	o	1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts (Dunning, 1993; Church and Hanks, 1990; Dagan et al. , 1999).
D07-1031	J93-2004	o	2 Evaluation All of the experiments described below have the same basic structure: an estimator is used to infer a bitag HMM from the unsupervised training corpus (the words of Penn Treebank (PTB) Wall Street Journal corpus (Marcus et al. , 1993)), and then the resulting model is used to label each word of that corpus with one of the HMMs hidden states.
N07-1049	W02-1001	o	For regularization purposes we adopt an average perceptron (Collins, 2002) which returns for each y, y = 1T summationtextTt=1 ty, the average of all weight vectors ty posited during training.
E09-1051	N06-1025	o	(Luo et al., 2004; Ponzetto and Strube, 2006) for other approaches with an evaluation based on true mentions only).
P07-1069	P04-1015	o	3For decoding, loc is averaged over the training iterations as in Collins and Roark (2004).
W05-0823	P02-1040	o	This algorithm adjusts the log-linear weights so that BLEU (Papineni et al. , 2002) is maximized over a given development set.
W08-0331	W05-0904	o	For instance, BLEU and ROUGE (Lin and Och, 2004) are based on n-gram precisions, METEOR (Banerjee and Lavie, 2005) and STM (Liu and Gildea, 2005) use word-class or structural information, Kauchak (2006) leverages on paraphrases, and TER (Snover et al., 2006) uses edit-distances.
P06-2077	P95-1026	o	These instances can be retagged with their countability by using the proposed method and some kind of bootstrapping (Yarowsky, 1995).
P09-1062	P08-1054	o	This obviously does not preclude using the audio-based system together with other features such as utterance position, length, speakers roles, and most others used in the literature (Penn and Zhu, 2008).
W05-1513	P97-1003	o	Table 1 shows a summary of the results of our experiments with SVMpar and MBLpar, and also results obtained with the Charniak (2000) parser, the Bikel (2003) implementation of the Collins (1997) parser, and the Ratnaparkhi (1997) parser.
P08-1059	D07-1091	o	Though our motivation is similar to that of Koehn and Hoang (2007), we chose to build an independent component for inflection prediction in isolation rather than folding morphological information into the main translation model.
W04-0212	J93-2004	p	1 Introduction Large scale annotated corpora such as the Penn TreeBank (Marcus et al. , 1993) have played a central role in speech and natural language research.
N06-1021	W02-1001	o	Also, we chose to average each individual perceptron (Collins, 2002) prior to Bayesian averaging.
A00-2030	P97-1003	o	7 Model Structure In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).
P08-2039	D07-1091	o	We also report on applying Factored Translation Models (Koehn and Hoang, 2007) for English-to-Arabic translation.
P03-1046	W96-0213	o	The input is POS-tagged using the tagger of Ratnaparkhi (1996).
W97-0209	J93-2004	o	Test and training materials were derived from the Brown corpus of American English, all of which has been parsed and manually verified by the Penn T~eebank project (Marcus et al. , 1993) and parts of which have been manually sense-tagged by the WordNet group (Miller et al. , 1993).
W00-1308	W96-0213	o	The features that define the constraints on the model are obtained by instantiation of feature templates as in Ratnaparkhi (1996).
P05-1057	J96-1002	p	An especially well-founded framework is maximum entropy (Berger et al. , 1996).
P03-1060	W95-0107	p	The simplest one is the BIO representation scheme (Ramshaw and Marcus, 1995), where a B denotes the first item of an element and an I any non-initial item, and a syllable with tag O is not a part of any element.
N07-1038	P02-1047	o	Since this relation can often be determined automatically for a given text (Marcu and Echihabi, 2002), we can readily use it to improve rank prediction.
C08-1094	J93-2004	o	Hence our classifier evaluation omits those two word positions, leading to n2 classifications for a string of length n. Table 1 shows statistics from sections 2-21 of the Penn WSJ Treebank (Marcus et al., 1993).
N04-1008	P02-1040	p	4.4.1 N-gram Co-Occurrence Statistics for Answer Extraction N-gram co-occurrence statistics have been successfully used in automatic evaluation (Papineni et al. 2002, Lin and Hovy 2003), and more recently as training criteria in statistical machine translation (Och 2003).
P06-2077	P95-1026	o	Yarowsky (1995) tested the claim on about 37,000 examples and found that when a polysemous word appeared more than once in a discourse, they took on the majority sense for the discourse 99.8% of the time on average.
I08-2116	H05-1087	o	Recently, methods for training binary classifiers to maximize the F 1 -score have been proposed for SVM (Joachims, 2005) and LRM (Jansche, 2005).
P03-1013	P97-1003	o	The Collins (1997) model does not use context-free rules, but generates the next category using zeroth order Markov chains (see Section 3.3), hence no information about the previous sisters is included.
I05-5001	N03-1003	o	A growing body of recent research has focused on the problems of identifying and generating paraphrases, e.g., Barzilay & McKeown (2001), Lin & Pantel (2002), Shinyama et al, (2002), Barzilay & Lee (2003), and Pang et al.
W08-0335	N03-1017	o	The training and decoding system of our SMT used the publicly available Pharaoh (Koehn et al., 2003)2.
W07-0403	J97-3002	o	Wu (1997) used a binary bracketing ITG to segment a sen19 tence while simultaneously word-aligning it to its translation, but the model was trained heuristically with a fixed segmentation.
N07-3003	N06-1025	o	and Semantic Knowledge Sources for Coreference Resolution Ponzetto & Strube (2006) and Strube & Ponzetto (2006) aimed at showing that the encyclopedia that anyone can edit can be indeed used as a semantic resource for research in NLP.
W98-1109	J92-4003	o	This is in contrast to work by researchers such as Schiitze and Pedersen (1992), Brown et al (1992) and Futrelle and Gauch (1995), where it is often the most frequent words in the lexicon which are clustered, predominantly with the purpose of determining their grammatical classes.
W08-0206	W96-0213	o	For Hw6, students compared their POS tagging results with the ones reported in (Ratnaparkhi, 1996).
P06-1006	H05-1083	o	In line with the reports in (Luo and Zitouni, 2005) we do observe the performance improvement against the baseline (NORM) for all the domains.
W03-1022	W02-1001	p	The averaged version of the perceptron (Collins, 2002), like the voted perceptron (Freund and Schapire, 1999), reduces the effect of over-training.
W09-2205	P07-1056	o	SCL for Discriminative Parse Selection So far, pivot features on the word level were used (Blitzer et al., 2006; Blitzer et al., 2007).
P04-1010	J99-3003	o	Adapting a vectorbased approach reported by Chu-Carroll and Carpenter (1999), the Task ID Frame Agent is domain-independent and automatically trained.
P08-2035	P97-1003	o	3 We then run Collins parser (1997), using just the sentence pairs where parsing succeeds with a negative log likelihood below 200.
N07-1022	N03-1017	o	3.1 Generation using PHARAOH PHARAOH (Koehn et al. , 2003) is an SMT system that uses phrases as basic translation units.
P06-2034	P97-1003	o	SCISSOR is implemented by augmenting Collins (1997) head-driven parsing model II to incorporate the generation of semantic labels on internal nodes.
C08-1114	J06-3003	o	The average senior high school student achieves 57% correct (Turney, 2006).
E09-3005	N06-1020	o	Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006).
N09-2038	P02-1040	o	Day 1 Day 2 No ASR adaptation 29.39 27.41 Unsupervised ASR adaptation 31.55 27.66 Supervised ASR adaptation 32.19 27.65 Table 2: Impact of ASR adaptation to SMT Table 2 shows the impact of ASR adaptation on the performance of the translation system in BLEU (Papineni et al., 2002).
P07-1106	W02-1001	o	Given an input sentence x, the correct output segmentation F(x) satisfies: F(x) = argmax yGEN(x) Score(y) where GEN(x) denotes the set of possible segmentations for an input sentence x, consistent with notation from Collins (2002).
W06-0301	J96-1002	o	As a learning algorithm for our classification model, we used Maximum Entropy (Berger et al. , 1996).
D09-1019	P02-1053	o	One of the main directions is sentiment classification, which classifies the whole opinion document (e.g., a product review) as positive or negative (e.g., Pang et al, 2002; Turney, 2002; Dave et al, 2003; Ng et al. 2006; McDonald et al, 2007).
D09-1150	J96-2004	o	3.1 Agreement for Emotion Classes The kappa coefficient of agreement is a statistic adopted by the Computational Linguistics community as a standard measure for this purpose (Carletta, 1996).
D08-1063	J96-1002	p	The classification is performed with a statistical approach, built around the maximum entropy (MaxEnt) principle (Berger et al., 1996), that has the advantage of combining arbitrary types of information in making a classification decision.
D07-1099	P07-1080	o	This curve plots the average labeled attachment score over Basque, Chinese, English, and Turkish as a function of parsing time per token.4 Accuracy of only 1% below the maximum can be achieved with average processing time of 17 ms per token, or 60 tokens per second.5 We also refer the reader to (Titov and Henderson, 2007b) for more detailed analysis of the ISBN dependency parser results, where, among other things, it was shown that the ISBN model is especially accurate at modeling long dependencies.
N01-1025	W95-0107	o	a176 Base NP standard data set (baseNP-S) This data set was first introduced by (Ramshaw and Marcus, 1995), and taken as the standard data set for baseNP identification task2.
W06-2915	W02-1011	o	So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al. , 2004; Riloff et al. , 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al. , 2003), and discriminating between positive and negative language (Pang et al. , 2002; Morinaga et al. , 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al. , 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al. , 2005).
J09-3004	J90-1003	p	Probably the most widely used feature weighting function is (point-wise) Mutual Information (MI) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch, Wang, and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovski and Pantel 2004; Pantel and Ravichandran 2004; Pantel, Ravichandran, and Hovy 2004; Weeds, Weir, and McCarthy 2004), dened by: weight MI (w,f)=log 2 P(w,f) P(w)P(f) (1) We calculate the MI weights by the following statistics in the space of co-occurrence instances S: weight MI (w,f)=log 2 count(w,f) nrels count(w) count(f) (2) where count(w,f) is the frequency of the co-occurrence pair w,f  in S, count(w)and count(f) are the independent frequencies of w and f in S,andnrels is the size of S.High MI weights are assumed to correspond to strong wordfeature associations.
J05-4004	P02-1057	o	In our own work on document compression models (Daume III and Marcu 2002; Daume III and Marcu 2004), both of which extend the sentence compression model of Knight and Marcu (2002), we assume that sentences and documents can be summarized exclusively through deletion of contiguous text segments.
J03-4003	W02-1002	o	(1994), and Magerman (1995) can suffer from very similar problems to the label bias or observation bias problem observed in tagging models, as described in Lafferty, McCallum, and Pereira (2001) and Klein and Manning (2002).
E09-1041	D07-1068	o	4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction (Ponzetto and Strube, 2007a), coreference resolution (Ponzetto and Strube, 2007b), and English NER (e.g., Bunescu and Pasca (2006), Cucerzan (2007), Kazama and Torisawa (2007), Watanabe et al.
N04-1021	P03-1021	o	However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003).
E09-3005	D07-1112	o	In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data.
P05-1075	J90-1003	o	METRIC FORMULA Frequency (Guiliano, 1964) x yf Pointwise Mutual Information [PMI] (Church & Hanks, 1990) ( )xy x y2log /P P P True Mutual Information [TMI] (Manning, 1999) ( )xy 2 xy x ylog /P P P P Chi-Squared ( 2 ) (Church and Gale, 1991) { }{ },, 2( ) i X X Y Y i j i j i j j f     T-Score (Church & Hanks, 1990) 1 2 2 2 1 2 1 2 x x s s n n  + C-Values4 (Frantzi, Anadiou & Mima 2000) 2 is not nested 2 log ( ) log ( ) 1 ( ) ( ) a a b T a f f f b P T         where is the candidate string f( ) is its frequency in the corpus T is the set of candidate terms that contain P(T ) is the number of these candidate terms 609 1,700 of the three-word phrases are attested in the Lexile corpus.
P09-1065	N03-1017	o	On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression phrase-based models.
P08-1021	P97-1003	o	For example, the sentence My father is *work in the laboratory is parsed (Collins, 1997) as: (S (NP My father) (VP is (NP work)) (PP in the laboratory)) 2The abbreviations s (is or has) and d (would or had) compound the ambiguities.
P06-1096	H05-1064	o	Discriminative training with hidden variables has been handled in this probabilistic framework (Quattoni et al. , 2004; Koo and Collins, 2005), but we choose Equation 3 for efficiency.
P98-2182	P95-1026	p	Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al. , 1992; Yarowsky, 1995).
W97-1006	J92-4003	o	Brown, (Brown et al. , 1992) uses the same bigrams and by means of a greedy algorithm forms the hierarchical clusters of words.
P09-1027	P07-1056	o	Training Set (Labeled English Reviews): There are many labeled English corpora available on the Web and we used the corpus constructed for multi-domain sentiment classification (Blitzer et al., 2007) 9 , because the corpus was large-scale and it was within similar domains as the test set.
P06-2014	J97-3002	p	The Inversion Transduction Grammar or ITG formalism, described in (Wu, 1997), is well suited for our purposes.
P03-1013	P97-1003	o	The lexicalized model proposed by Collins (1997) (henceforth Collins model) was re-implemented by one of the authors.
W08-0304	P03-1021	o	Och (2003) claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective, O = lscript.
J05-4003	J93-2003	o	One such model is the IBM Model 1 (Brown et al. 1993).
J07-2003	P02-1040	o	Our evaluation metric is case-insensitive BLEU-4 (Papineni et al. 2002), as defined by NIST, that is, using the shortest (as opposed to closest) reference sentence length for the brevity penalty.
D09-1066	C02-1007	o	3 Methodology Similar to (Rapp, 2002; Baroni et al., 2008, among others), we use comparison to human assocation datasets as a test bed for the scores produced by computational association measures.
E91-1038	P90-1034	p	Semantic collocations are harder to extract than cooccurrence patterns--the state of the art does not enable us to find semantic collocations automatically t. This paper however argues that if we take advantage of lexicai paradigmatic behavior underlying the lexicon, we can at least achieve semi-automatic extraction of semantic collocations (see also Calzolari and Bindi (1990) I But note the important work by Hindle \[HindlegO\] on extracting semantically similar nouns based on their substitutability in certain verb contexts.
W97-0205	J90-1003	o	The classifier uses mutual information (MI) scores rather than the raw frequences of the occurring patterns (Church and Hanks, 1990).
N09-1009	D07-1031	o	There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007).
N04-1008	J93-1003	o	The chunker is trained on the answer side of the Training corpus in order to learn 2 and 3word collocations, defined using the likelihood ratio of Dunning (1993).
D07-1005	P06-1097	o	We observe that AER is loosely correlated to BLEU ( = 0.81) though the relation is weak, as observed earlier by Fraser and Marcu (2006a).
W06-0304	P04-1035	o	The focus of much of the automatic sentiment analysis research is on identifying the affect bearing words (words with emotional content) and on measurement approaches for sentiment (Turney & Littman, 2003; Pang & Lee, 2004; Wilson et al. , 2005).
P08-1089	N03-1003	o	The preci781 start Palestinian suicide bomberblew himself up in SLOT1 on SLOT2 killing SLOT3 other people and injuring wounding SLOT4 end detroit the *e* a s *e* building buildingin detroit flattened ground levelled to blasted leveled *e* was reduced razed leveled to down rubble into ashes *e* to *e* (1) (2) Figure 1: Examples of paraphrase patterns extracted by Barzilay and Lee (2003) and Pang et al.
W01-0706	P97-1003	p	For the full parser, we use the one developed by Michael Collins (Collins, 1996; Collins, 1997)  one of the most accurate full parsers around.
P06-2004	P97-1003	o	6 Related Work Other work combining supervised and unsupervised learning for parsing includes (Charniak, 1997), (Johnson and Riezler, 2000), and (Schmid, 2002).
W05-1514	W96-0213	o	4 Filtering with the CFG Rule Dictionary We use an idea that is similar to the method proposed by Ratnaparkhi (Ratnaparkhi, 1996) for partof-speech tagging.
D09-1054	W06-1643	o	The skip-chain CRFs (Sutton and McCallum, 2004; Galley, 2006) model the long distance dependency between context and answer sentences and the 2D CRFs (Zhu et al., 2005) model the dependency between contiguous questions.
C08-2004	P04-1035	o	Within NLP, applications include sentiment-analysis problems (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006) and content selection for text generation (Barzilay and Lapata, 2005).
D07-1099	P07-1080	p	We use a recently proposed dependency parser (Titov and Henderson, 2007b)1 which has demonstrated state-of-theart performance on a selection of languages from the 1The ISBN parser will be soon made downloadable from the authors web-page.
D07-1102	D07-1014	o	We can sum over all non-projective spanning trees by taking the determinant of the Kirchhoff matrix of the graph defined above, minus the row and column corresponding to the root node (Smith and Smith, 2007).
X98-1014	P97-1003	o	Statistical Model In SIFT's statistical model, augmented parse trees are generated according to a process similar to that described in Collins (1996, 1997).
C00-2124	J96-1002	o	For every class the weights of the active features are combined and the best scoring class is chosen (Berger et al. , 1996).
W06-3104	J93-2003	o	1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 15 (Brown et al. , 1993).
W09-0418	D07-1091	o	A tight integration of morphosyntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation.
D09-1022	D07-1007	o	Another WSD approach incorporating context-dependent phrasal translation lexicons is given in (Carpuat and Wu, 2007) and has been evaluated on several translation tasks.
P07-1058	N03-1003	o	Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al. , 2002; Barzilay and Lee, 2003; Pang et al. , 2003; Szpektor et al. , 2004; Sekine, 2005).
W06-1615	J92-4003	o	There are many choices for modeling co-occurrence data (Brown et al. , 1992; Pereira et al. , 1993; Blei et al. , 2003).
P08-3001	P90-1034	o	A number of researches which utilized distributional similarity have been conducted, including (Hindle, 1990; Lin, 1998; Geffet and Dagan, 2004) and many others.
W00-1308	W96-0213	n	"Ratnaparkhi (1996: 134) suggests use of an approximation summing over the training data, which does not sum over possible tags: "" h E f j = 2 P( ~)p(ti l hi)f j(hi,ti) i=1 However, we believe this passage is in error: such an estimate is ineffective in the iterative scaling algorithm."
W04-0846	P95-1026	o	These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998].
P07-1004	J04-3004	o	Under certain precise conditions, as described in (Abney, 2004), we can analyze Algorithm 1 as minimizing the entropy of the distribution over translations of U. However, this is true only when the functions Estimate, Score and Select have very prescribed definitions.
W06-2903	P05-1010	p	Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al. , 2005) conditioning information.
W06-3115	P03-1021	o	First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004).
D09-1059	J93-2004	p	In the statistical NLP community, the most widely used grammatical resource is the Penn Treebank (Marcus et al., 1993).
C04-1005	J97-3002	o	In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments.
A00-2036	P97-1003	n	Bilexical context-free grammars have been presented in (Eisner and Satta, 1999) as an abstraction of language models that have been adopted in several recent real-world parsers, improving state-of-the-art parsing accuracy (A1shawl, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997).
N06-1004	N03-1017	o	Phrase tables were learned from the training corpus using the diag-and method (Koehn et al. , 2003), and using IBM model 2 to produce initial word alignments (these authors found this worked as well as IBM4).
C04-1134	W96-0213	o	We use two state-of-the-art POS taggersa maximum entropy based English POS tagger (Ratnaparkhi, 1996), and an HMM based Chinese POS tagger.
P09-1054	W02-1001	o	Following (Collins, 2002), we used sections 0-18 of the Wall Street Journal (WSJ) corpus for training, sections 19-21 for development, and sections 22-24 for final evaluation.
D07-1058	J93-2004	o	For a second set of parsing experiments, we used the WSJ portion of the Penn Tree Bank (Marcus et al. , 1993) and Helmut Schmids enrichment program tmod (Schmid, 2006).
J94-4003	P93-1022	p	A promising approach may be to use aligned bilingual corpora, especially for augmenting existing lexicons with domain-specific terminology (Brown et al. 1993; Dagan, Church, and Gale 1993).
W09-0405	W05-0909	o	3.3 System evaluation Since both the system translations and the reference translations are available for the tuning 43 set, we first compare each output to the reference translation using BLEU (Papineni et al., 2001) and METEOR (Banerjee and Lavie, 2005) and a combined scoring scheme provided by the ULC toolkit (Gimenez and Marquez, 2008).
W08-0409	J97-3002	o	Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006).
C08-1111	W02-1011	o	2 Related Work Recently, several studies have reported about dialog systems that are capable of classifying emotions in a human-computer dialog (Batliner et al., 2004; Ang et al., 2002; Litman and Forbes-Riley, 2004; Rotaru et al., 2005).
P04-1086	W02-1001	o	Among these methods, CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging (Lafferty et al. , 2001), Named-Entity Recognition (Collins, 2002) and shallow parsing (Sha and Pereira, 2003; McCallum, 2003).
W02-1022	P02-1040	o	While recent proposals for evaluation of MT systems have involved multi-parallel corpora (Thompson and Brew, 1996; Papineni et al. , 2002), statistical MT algorithms typically only use one-parallel data.
P06-2081	P02-1053	o	Work focusses on analysing subjective features of text or speech, such as sentiment, opinion, emotion or point of view (Pang et al. , 2002; Turney, 2002; Dave et al. , 2003; Liu et al. , 2003; Pang and Lee, 2005; Shanahan et al. , 2005).
C00-1052	P97-1003	o	Ultinmtely, however, it seems that a more complex ai)t)roach incorporating back-off and smoothing is necessary ill order to achieve the parsing accuracy achieved by Charniak (1997) and Collins (1997).
I05-3005	W96-0213	o	(2002), who retrain the Ratnaparkhi (1996) tagger and reach accuracies of 93% using CTB-I.
A00-1019	J93-2003	o	3.2 Mapping Mapping the identified units (tokens or sequences) to their equivalents in the other language was achieved by training a new translation model (IBM 2) using the EM algorithm as described in (Brown et al. , 1993).
P05-1032	P02-1040	o	We calculated the translation quality using Bleus modified n-gram precision metric (Papineni et al. , 2002) for n-grams of up to length four.
C02-1085	W00-0405	o	More recently, other approaches have investigated the use of machine learning to nd patterns in documents(Strzalkowski et al. , 1998) and the utility of parameterized modules so as to deal with dierent genres or corpora(Goldstein et al. , 2000).
P95-1039	A92-1018	p	1 Motivation Statistical part-of-speech disambiguation can be efficiently done with n-gram models (Church, 1988; Cutting et al. , 1992).
D09-1071	D07-1031	o	Recent work (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) on this task explored a variety of methodologies to address this issue.
C00-2102	W95-0107	p	This means that the 1)roblem of recognizing named entities in those cases can be solved by incorporating techniques of base noun phrase chunking (Ramshaw and Marcus, 1995).
W97-0812	P95-1026	o	(1992), Yarowsky (1995), and Karol & Edelman (1996) where strong reliance on statistical techniques for the calculation of word and context similarity commands large source corpora.
W03-1020	J96-1002	o	1 Introduction Maximum Entropy (ME) modeling has received a lot of attention in language modeling and natural language processing for the past few years (e.g. , Rosenfeld, 1994; Berger et al 1996; Ratnaparkhi, 1998; Koeling, 2000).
D07-1073	N06-1025	o	In fact, many studies that try to exploit Wikipedia as a knowledge source have recently emerged (Bunescu and Pasca, 2006; Toral and Munoz, 2006; Ruiz-Casado et al. , 2006; Ponzetto and Strube, 2006; Strube and Ponzetto, 2006; Zesch et al. , 2007).
N01-1023	J93-2004	o	6 Experiment 6.1 Setup The experiments we report were done on the Penn Treebank WSJ Corpus (Marcus et al. , 1993).
C02-1065	J93-1003	o	As the strength of relevance between a target compound noun t and its co-occurring word r, the feature value of r, w(t;r) is deflned by the log likelihood ratio (Dunning, 1993) 1 as follows.
W03-0403	P97-1003	o	Most statistical parsing research, such as Collins (1997), has centered on training probabilistic context-free grammars using the Penn Treebank.
P06-2020	J93-1003	o	To identify these terms,weusethelog-likelihoodstatisticsuggested by Dunning (Dunning 1993) and first used in summarization by Lin and Hovy (Hovy and Lin 2000).
W09-0416	W96-0213	o	The features we used are as follows:  Direct and inverse IBM model;  3, 4-gram target language model;  3, 4, 5-gram POS language model (Ratnaparkhi, 1996; Schmid, 1994); 96  Sentence length posterior probability (Zens and Ney, 2006);  N-gram posterior probabilities within the NBest list (Zens and Ney, 2006);  Minimum Bayes Risk probability;  Length ratio between source and target sentence; The weights are optimized via MERT algorithm.
J98-1001	J92-4003	o	A similar view underlies the class-based methods cited in Section 2.4.3 (Brown et al. 1992; Pereira and Tishby 1992; Pereira, Tishby, and Lee 1993).
W93-0113	J92-4003	o	\[Brown et al. , 1992\] Peter F. Brown, Vincent J. Della Pietra, Petere V. deSouza, Jenifer C. Lai, and Robert L. Mercer.
D07-1111	J93-2004	o	In the multilingual parsing track, participants train dependency parsers using treebanks provided for ten languages: Arabic (Hajic et al. , 2004), Basque (Aduriz et al. 2003), Catalan (Mart et al. , 2007), Chinese (Chen et al. , 2003), Czech (Bhmova et al. , 2003), English (Marcus et al. , 1993; Johansson and Nugues, 2007), Greek (Prokopidis et al. , 2005), Hungarian (Czendes et al. , 2005), Italian (Montemagni et al. , 2003), and Turkish (Oflazer et al. , 2003).
A00-1031	W96-0213	o	The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996).
N07-1061	N03-1017	o	For details on these feature functions, please refer to (Koehn et al. , 2003; Koehn, 2004; Koehn et al. , 2005).
W06-1652	P04-1035	o	3 Data Sets We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al. , 2004), the Polarity data set5 created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al. , 2005).6 The OP and Polarity data sets involve document-level opinion classi cation, while the MPQA data set involves 5Version v2.0, which is available at:  6Available at  sentence-level classi cation.
D08-1028	P02-1040	o	There are however other similarity metrics (e.g. BLEU (Papineni et al., 2002)) which could be used equally well.
C08-1114	J06-3003	o	Turney (2006) later addressed the same problem using 8000 automatically generated patterns.
C08-1041	D07-1007	o	Furthermore, they extended WSD to phrase sense disambiguation (PSD) (Carpuat and Wu, 2007a).
P04-1077	W04-1013	p	ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).
D08-1039	J93-2003	o	3 Model As an extension to commonly used lexical word pair probabilities p(f|e) as introduced in (Brown et al., 1993), we define our model to operate on word triplets.
W08-0504	J96-1002	p	(2006), but we use a maximum entropy classifier (Berger et al., 1996) to determine parser actions, which makes parsing considerably faster.
P09-2026	D07-1001	o	4.1 Corpora Sentence compression systems have been tested on product review data from the Ziff-Davis (ZD, henceforth) Corpus by Knight and Marcu (2000), general news articles by Clarke and Lapata (CL, henceforth) corpus (2007) and biomedical articles (Lin and Wilbur, 2007).
W99-0604	J93-2003	o	This alignment representation is a generalization of the baseline alignments described in (Brown et al. , 1993) and allows for many-to-many alignments.
D07-1003	J93-2003	o	This sort of problem can be solved in principle by conditional variants of the Expectation-Maximization algorithm (Baum et al. , 1970; Dempster et al. , 1977; Meng and Rubin, 1993; Jebara and Pentland, 1999).
D08-1105	P07-1033	p	5.1 The AUGMENT technique for Domain Adaptation The AUGMENT technique introduced by Daume III (2007) is a simple yet very effective approach to performing domain adaptation.
J98-1006	P95-1026	o	Yarowsky (1995) has proposed automatically augmenting a small set of experimenter-supplied seed collocations (e.g. , manufacturing plant and plant life for two different senses of the noun plant) into a much larger set of training materials.
I08-1067	N03-1017	p	Phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases, the joint model, and IBM model 4 (Koehn et al., 2003).
W06-1314	J96-1002	o	96 Research on DA classification initially focused on two-party conversational speech (Mast et al. , 1996; Stolcke et al. , 1998; Shriberg et al. , 1998) and, more recently, has extended to multi-party audio recordings like the ICSI corpus (Shriberg et al. , 2004).
W06-2902	J93-2004	o	We use the Penn Treebank Wall Street Journal corpus as the large corpus and individual sections of the Brown corpus as the target corpora (Marcus et al. , 1993).
P06-2065	J93-2003	o	Machine translation has code-like characteristics, and indeed, the initial models of (Brown et al. , 1993) took a word-substitution/transposition approach, trained on a parallel text.
N06-1026	J96-1002	o	Maximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible (Berger et al. 1996).
D09-1075	P02-1040	o	For practical reasons, the maximum size of a token was set at three for Chinese, andfour forKorean.2 Minimum error rate training (Och, 2003) was run on each system afterwardsand BLEU score (Papineni et al., 2002) was calculated on the test sets.
P98-1119	J92-4003	o	In some cases, class (or part of speech) n-grams are used instead of word n-grams(Brown et al. , 1992; Chang and Chen, 1996).
W02-0808	J96-2004	o	We chose nouns that occur a minimum of 10 times in the corpus, have no undetermined translations and at least five different translations in the six nonEnglish languages, and have the log likelihood score of at least 18; that is: LL(T T, T S ) =  = 2 1 ij n* j * j*i ij n log  18 where n ij stands for the number of times T T and T S have been seen together in aligned sentences, n i* and n *j stand for the number occurrences of T T and T S, respectively, and n ** represents the total 4 We computed raw percentages only; common measures of annotator agreement such as the Kappa statistic (Carletta, 1996) proved to be inappropriate for our two-category (yesno) classification scheme.
W09-1104	N06-1020	o	Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training (Pereira and Schabes, 1992; Klein and Manning, 2004; Smith and Eisner, 2004; Smith and Eisner, 2005) over bootstrapping approaches like selftraining (McClosky et al., 2006) to feature-based enhancements of discriminative reranking models (Koo et al., 2008) and the application of semisupervised SVMs (Wang et al., 2008).
P98-2184	J93-2004	o	1 Introduction The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers (Charniak 1995, Collins 1996/1997, Joshi and Srinivas 1994, Kim, Srinivas, and Trueswell 1997, Stolcke et al. 1997), and in psychological theories of language processing (Clifton et al. 1984, Ferfeira & McClure 1997, Gamsey et al. 1997, Jurafsky 1996, MacDonald 1994, Mitchell & Holmes 1985, Tanenhaus et al. 1990, Trueswell et al. 1993).
N09-1022	J96-1002	o	3.5 Maximum Entropy Model In order to build a unified probabilistic query alteration model, we used the maximum entropy approach of (Beger et al., 1996), which Li et al.
W05-0836	P03-1021	o	In this paper we will compare and evaluate several aspects of these techniques, focusing on Minimum Error Rate (MER) training (Och, 2003) and Minimum Bayes Risk (MBR) decision rules, within a novel training environment that isolates the impact of each component of these methods.
W97-0709	J90-1003	o	"s e, the window to consider when extracting words related to word w, should span from postttuon w-5 to w+5 Maarek also defines the resolwng power of a parr m a document d as P = ~'Pd log Pc where Pd is the observed probabshty of appearance of the pan"" m document d, Pc the observed probabdny of the pmr recorpus, and -log Pc the quantity of mformauon assocmted to the pmr It Is easdy seen that p wall be h|gher, the higher the frequency of the pmr m the document and the lower sts frequency m the corpus, which agrees wlth the sdea presented at the begmnmg of this sectton Church and Hanks (1990) propose the apphcatlon of the concept of mutual mformatton e(x,y) ~,(x.y) = hog2 ecx)e(y) 51 to the retrieval, ro a corpus, of pairs of lextcally related words They alsoconslder a word span of :e5 words and observe that ""roterestrog"" pmr, s generally present a mutual mformatxon above 3 Salton and.Allan (1995) foc~as on paragraph level Each paragraph Is represented by a weighed vector, where each element is a term (typically."
W09-2406	J93-2004	o	3 Network Evaluation We present an evaluation which has been carried out on an initial set of annotations of English articles from The Wall Street Journal (covering those annotated at the syntactic level in the Penn Treebank (Marcus et al., 1993)).
N07-2048	P02-1053	o	Turney (2002) has presented an unsupervised opinion classification algorithm called SO-PMI (Semantic Orientation Using Pointwise Mutual Information).
W06-3119	N03-1017	o	138 2 Rule Generation We start with phrase translations on the parallel training data using the techniques and implementation described in (Koehn et al. , 2003a).
W03-1002	J93-2003	o	POS tagging and phrase chunking in English were done using the trained systems provided with the fnTBL Toolkit (Ngai and Florian, 2001); both were trained from the annotated Penn Treebank corpus (Marcus et al. , 1993).
P07-1036	N06-1041	o	(Grenager et al. , 2005) and (Haghighi and Klein, 2006) also report results for semi-supervised learning for these domains.
W08-0409	W96-0213	o	We tagged all the sentences in the training and devset3 using a maximum entropy-based POS tagger MXPOST (Ratnaparkhi, 1996), trained on the Penn English and Chinese Treebanks.
P08-2054	P05-1010	o	Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks (Henderson, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006).
W98-1109	J92-4003	o	While previous researchers have used agglomerative nesting clustering (e.g. Brown et al (1992), Futrelle and Gauch (1993)), comparisons with our work are difficult to draw, due to their use of the 1,000 commonest words from their respective corpora.
J96-2003	J92-1002	o	Illustrative clusterings of this type can also be found in Pereira, Tishby, and Lee (1993), Brown, Della Pietra, Mercer, Della Pietra, and Lai (1992), Kneser and Ney (1993), and Brill et al.
W05-0822	P02-1040	o	Once this is accomplished, a variant of Powells algorithm is used to find weights that optimize BLEU score (Papineni et al, 2002) over these hypotheses, compared to reference translations.
W02-0402	W00-0405	o	The marginal relevance systems (MR and MR+IE) used a simple selection mechanism which does not involve search, inspired by the maximal marginal relevance (MMR) approach (Goldstein et al. , 2000).
D09-1160	J96-1002	o	lscript1-regularized log-linear models (lscript1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assumingaLaplacianpriorontheweights(Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007).
P97-1061	J93-1007	o	(Smadja, 1993) proposed a method to retrieve collocations by combining bigrams whose cooccurrences are greater than a given threshold 3.
P09-1024	P08-1092	o	For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsyetal.,2008),whileothersgobeyondbinary distinction by identifying atomic events  e.g., occupation and marital status  that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006).
P09-1057	D07-1031	o	The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed.
P08-1078	P07-1088	o	(Downey et al., 2007) use HMM-based similarity for the same purpose.
P06-1121	J97-3002	o	7 Related work Similarly to (Poutsma, 2000; Wu, 1997; Yamada and Knight, 2001; Chiang, 2005), the rules discussed in this paper are equivalent to productions of synchronous tree substitution grammars.
N09-1068	P07-1033	o	5 Related Work We already discussed the relation of our work to (Daume III, 2007) in Section 2.4.
J06-4004	P02-1040	o	In our SMT system implementation, this optimization procedure is performed by using a tool developed in-house, which is based on a simplex method (Press et al. 2002), and the BLEU score (Papineni et al. 2002) is used as a translation quality measurement.
P03-1013	P97-1003	o	Prominent among these properties is the semi-free Language Size LR LP Source English 40,000 87.4% 88.1% (Collins, 1997) Chinese 3,484 69.0% 74.8% (Bikel and Chiang, 2000) Czech 19,000 80.0% (Collins et al. , 1999) Table 1: Results for the Collins (1997) model for various languages (dependency precision for Czech) wordorder, i.e., German wordorder is fixed in some respects, but variable in others.
D08-1060	N06-1033	o	Work in (Al-Onaizan and Kishore, 2006; Xiong et al., 2006; Zens et al., 2004; Kumar and Byrne, 2005; Tillmann and Zhang, 2005) modeled the limited information available at phrase-boundaries.
C08-1026	P08-1085	o	Even for many unsupervised situations, this is available from a lexicon (e.g., Banko and Moore, 2004; Goldberg et al., 2008).
C00-1064	J96-1002	o	4 Maximum Entropy To explain our method, we l)riefly des(:ribe the con(:ept of maximum entrol)y. Recently, many al)lnoaches l)ased on the maximum entroi)y lnodel have t)een applied to natural language processing (Berger eL al. , \]994; Berger et al. , 1996; Pietra et al. , 1997).
H94-1034	J93-2004	o	In a test set of 756 utterances containing 26 repairs (Dowding et al. , 1993), they obtained a detection recall rate of 42% and a precision of 84.6%; for correction, they obtained a recall rate of 30% and a precision rate of 62%.
P97-1039	J93-2003	o	Bitexts also play a role in less automated applications such as concordancing for bilingual lexicography (Catizone et al. , 1993; Gale & Church, 1991b), computer-assisted language learning, and tools for translators (e.g.
W99-0505	C94-2113	o	"Towards a Meaning-Full Comparison of Lexieal Resources Kenneth C Lltkowska CL Research 9208 Gue Road Damascus, MD 20872 ken@clres corn  tires tom Abstract The mapping from WordNet to Hector senses m Senseval provides a ""gold standard"" against wluch to judge our ability to compare lexlcal resources The ""gold standard"" is provided through a word overlap analysis (with and without a stop list) for flus mapping, achieving at most a 36 percent correct mapping (inflated by 9 percent from ""empty"" assignments) An alternaUve componenttal analysis of the defimtaons, using syntacUc, collocatmnal, and semantac component and relation identification (through the use ofdefimng patterns integrated seamlessly mto the parsing thclaonary), provides an almost 41 percent correct mapping, with an additaonal 4 percent by recogmzmg semantic components not used in the Senseval mapping Defimtion sets of the Senseval words from three pubhshed thclaonanes and Dorr's lextcal knowledge base were added to WordNet and the Hector database to exanune the nature of the mapping process between defimtton sets of more and less sco\[~e The tecbauques described here consUtute only an maaal implementation of the componenUal analysis approach and suggests that considerable further improvements can be aclueved Introduction The difficulty of companng lemcal resources, long a s~gnfficant challenge in computauonal hnguistlcs (Atlans, 1991), came to the fore in the recent Senseval competatton (IOlgarnff, 1998), when some systems that relied heavily on the WordNet (Miller, et al, 1990) sense inventory were faced with the necessity of using another sense inventory (Hecto0 A hasty solutaon to the problem was the "" development of a map between the two inventories, but some part~cipants expressed concerns that use of flus map may have degraded their performance to an unknown degree Although there were disclaimers about the WordNet-Hector map, it nonetheless stands as a usable gold standard for efforts to compare lexical resources Moreover, we have a usable baseline (a word overlap method suggested m (Lesk, 1986)) against which to compare whether we are able to make improvements m the mapping (since flus method has been shown to perform not as well as expected (Krovetz, 1992)) We first describe the lextcal resources used m the study (Hector, WordNet, other dicUonanes, and a lex~cal knowledge base), first characterizing them in terms ofpolysemy and the types of leracal mformaUon each contmns (syntacUc properties and features, semantac components and relaUons, and collocaUonal properties) We then present results of perfornung the word overlap analysis of the 18 verbs used m Senseval, analyzing the definitions m WordNet and Hector We then expand our analysis to include other dictionaries We describe our methods of analysis, particularly the methods of parsing defimtaons and identff)qng semantic relations (semrels) based on defimng patterns, essentially takang first steps m Implementing the program described by Atkms and focusmg on the use of""meamng"" full mformataon rather than statistical mformaUon We identify the results that have been achieved thus far and outline further steps that may add more ""meanmg"" to the analysis IAll analyses described m this paper were performed automatically using functlonahty incorporated m DIMAP (Dictionary Maintenance Programs) (available for immediate download at (CL Research, 1999a)) This includes automatac extracuon of WordNet reformation for the selected words (mtegrated m DIMAP) Hector defimtlons were uploaded into DIMAP dicUonanes after use of a conversmn program Defimtlons for other 30 The Lexical Resources Tlus analysis focuses on the mmn verb senses used In Senseval (not ichoms and phrases), specifically the followmg AMAZE, BAND, BET, BOTHER, BURY, CALCULATE, CONSUME, DERIVE, FLOAT, HURDLE, INVADE, PROMISE, SACK, SANCTION, SCRAP, SEIZE, SHAKE, SLIGHT The Hector database used In Senseval consists of a tree of senses, each of which contains defimttons, syntactic properties, example usages, and ""clues"" (collocational information about the syntactic and semantic enwronment in wluch a word appears in the spectfic sense) The WordNet database contmns synonyms (synsets), perhaps a defimtton or example usages (gloss), some syntactic mformaUon (verb frames), hypernyms, hyponyms, and some other semrels (ENTAILS, CAUSES) To extend our analysis In order to look at other issues of lexacal resource comparison, we have included the defirauons or leracal information from the following additional sources  Webster's 3 ra New International Dictionary (W3)  Oxford Advanced l.earners D~ctlonary (OALD)  American Hentage DlcUonary (AI-ID)  Dorr's Lexacal Knowledge Base (Dorr) We used only the defimuons from W3, OALD, and AHD (which also contmn sample usages and some collocattonal information m the form of usage notes, not used at the present tame) Dorr's database contains thematic grids wluch characterize the thematic roles of obligatory and optional semanuc components, frequently identifying accompanying preposmons (Olsen, et al, 1998) The following table identities the number of senses and average overall polysemy for each of these resources dictionaries were entered by hand Word amaze band bet bother bury calculate consume denve float hurdle invade pronuse sack sanction scrap seize shake shght Average Polysemy o o o 1 2 4 2 3 1 II 4 4 2 5 5 7 6 9 7 12 6 14 5 5 5 10 9 6 6 8 8 6 5 15 5 16 4 41 14 2 1 4 3 6 2 10 5 5 4 7 4 4 4 6 3 2 2 5 2 3 1 3 3 11 6 21 13 8 8 37 17 1 1 6 3 O 1 2 2 4 1 3 4 4 8 1 3 1 3 1 3 2 10 5 1 0 3 1 3 2 2 0 1 1 1 0 7 1 7 12 I 0 57 37 120 62 34 22 Word Overlap Analysis We first estabhsh a baseline for automatic replication of the lexicographer's mappmg from WordNet 1 6 to Hector, using a s~mple word overlap analysis smular to (Lesk, 1986) The lextcographer mapped the 66 WordNet senses (each synset m which a test occurred) Into 102 Hector senses A total of 86 assignments were made, 9 WordNet senses were gwen no assignments, 40 recewed exactly one, and 17 senses received 2 or 3 asssgnments The WordNet senses contained 348 words (about half of wluch were common words appeanng on our stop list, which contained 165 words, mostly preposmons, pronouns, and conjunctions) The Hector senses selected m the word overlap analysis contained about 960 words (all Hector senses contained 1878 words) We performed a strict word overlap analysts (with and wsthout a stop hst) between tile definlUons in WordNet and the Hector senses, that is, we did not attempt to ldenttfy root forms of Inflected words We took each word m a WordNet sense and determined whether ~t appeared in a Hector sense, we selected a Hector sense based on the highest percentage of words over all Hector senses An 31 empty selection was made ff all the words in the WordNet sense did not appear in any Hector sense, only content words were considered when the stop hst was used For example, for bet, WordNet sense 2 (stake (money) on the outcome of an issue) mapped into Hector sense 4 ((of a person) to risk (a sum of money or property) m thts way) In this case, there was an overlap on two words (money, 039 in the Hector defimtlon (0 13 of its 15 words) without the stop list When the stop list was invoked, there was an overlap of only one word (money, 0 07 of the Hector defimtion) In this case, the lexicographer had made three assignments (Hector senses 2, 3, and 4), our scoring method treated flus as only 1 out of 3 correct (not using the relaxed method employed in Senseval of treating flus as completely correct) Without the stop hst, our selections matched the lexicographer's in 28 of 86 cases (32 6%), using the stop list, we were successful in 31 of 86 cases (36 1%) The improvement arising when the stop list was used is deceptive, where 8 cases were due to empty assignments (so that only 23 cases, 26 7%, were due to matching content words) Overall, only 41 content words were involved in these 23 successes when the stop list was used, an average of I 8 content words To summanze the word overlap analysis (1) despite a ncher set of defimtions in Hector, 9 of 66 WordNet senses (13 6%) could not be assigned, (2) despite the greater detail in Hector senses compared to WordNet senses (2 8 times as many words), only 1 8 content words participated in the assignments, and (3) therefore, the defimng vocabulary between these two definition sets seems to be somewhat divergent Although it might appear as if the word overlap analysis does not perform well, this is not the case The analysis provides a broad overview of the defimuon companson process between two definmon sets and frames a deeper analysis of the differences Moreover, it appears that the accuracy of a ""gold standard"" mapping is not crucially important The quality of the mapping may help frame the subsequent analysis more precisely, but it seems sufficient that any reasonable mapping will suffice This will be discussed further after presenting the results of the componentlal analysis of the defimtlons 32 Meaning-Full Analysis of Definitions The deeper analysis of the mapping between two defimtion sets relies primarily on two major steps (1) parsing definitions and using defimng patterns to identify semrels present m the definitions and (2) relaxing values to these relations by allowing ""synonymic"" substitution (using WordNet) Thus, for example, ffwe identify hypernyms or instruments from parsing a defimtion, we would say that the defimtions are ""equal"" not just ffthe hypernym or instrument is the same word, but also Lf the hypernyms or instruments are members of the same synset This approach is based on the finding (Litkowski, 1978) that a dictionary induces a semantic network where nodes represent ""concepts"" that may be lexicahzed and verbalized in more than one way This finding implies, in general, the absence of true synonyms, and instead the kind of ""concept"" embodied in WordNet synsets (with several lexical items and phraseologles) A slmdar approach, parsing defimtlons and relaxing semrel values, was followed in (Dolan, 1994) for clnstenng related senses w~thin a single dictionary The ideal toward which this approach strives is a complete identification of the meamng components included in a defimtion The meaning components can include syntactic features and charactenstlcs (including subcategonzation patterns), semantm components (realized through identification of semrels), selectional restrictions, and coUocational specifications The first stage of the analysis parses the definitions (CL Research, 1999b, Litkowski, to appear) and uses the parse results to extract (via defining patterns) semrels Since definitions have many idiosyncrasies (that do not follow ordinary text), an important first step in this stage is preprocessmg the definition text to put it into a sentence frame that facilitates the extraction of semrels 2 2Note that the stop hst is not applicable to the definition parsing The parser is a full-scale sentence parser, where prepositmns and other words on the stop list are necessary for successful parsing Moreover, inclusion of the prepositions is cmcml to the method, since they are the bearers of much semrel information The extractmn of semrels examines the parse results, a e, a tree whose mtermedaate nodes represent non-ternunals and whose leaves represent the lextcal atems that compnse the defimuons, where any node may also include annotations such as characterizations of number and tense For all noun or verb defimttons, flus includes Identification of the head noun (with recogmtton of""empty"" heads) or verb, for verbs, we signal whether the defimtaon contmned any selecttonal restnctmus (that as, pamcular parenthesazed expressaons) for the subject and object We then exanune preposattonal phrases In the defimUon and deterrmne whether we have a ""defining pattern"" for the preposaUon whach we can use as mdacaUve of a partacular semrel We also identify adverbs m the parse tree and look these up in WordNet to adentffy an adjecuve synset from wluch they are derived (if one is gwen) The defimng pattems are actually part of the dictionary used by the parser That is, we do not have to develop specafic routines to look for specLfic patterns A defimng pattern ~s a regular expressaon that arlaculates a syntactac pattern to be matched Thus, to recograze a ""manner"" semrel, we have the foUowmg entry for ""m"" m(dpat((~ rep0 l(det(0)) adj manner(0) st(manner)))) This allows us to recognize ""m"" as possibly gwmg rise to a ""manner"" component, where we recogmze ""m"" (the tdde, which allows us to specify partacular elements before the ""m"" as well), vath a noun phrase that consasts of 0 or 1 determiner, an adjectwe, and the lateral ""manner"" The '0  after the detenmner and the hteral mdacate that these words are not copied into the value for a ""manner"" role, so that the value to the ""manner"" semrel becomes only the adjectwe that as recogmzed The second stage of the analysis uses the populated lexacal database to compare senses and make the selectaons This process follows the general methodology used m Senseval (Lltkowska, to appear) Specifically, m the defimtaon comparison, we first exanune exclusaon cntena to rule out specific mappings These criteria include syntacUc properUes (e g, a verb sense that Is only transluve cannot map into one that Is only mtransRave) and collocataonal propertaes (e g, a sense that is used with a parUcle cannot map into one that uses a different particle) At the present tune, these are used only rmmmally 33 We next score each viable sense based on rots semrels We increment the score ff the senses have a common hypernym or If a sense's hypernyms belong  to the same synset as the other sense's hypernyms If a parUcular sense con~ns a large number of synonyms (that as, no differentiae on the hypernym) and they overlap consaderably m the synsets they evoke, the score can be increased substanUally Currently, we add 5 points for each match 3 We increment the score based on common semrels In tins amtml tmplementaUon, we have defimng patterns (usually qmte nummal) for recogmzmg Instrument, means, location, purpose, source, manner, has-constituents, has-members, is-part-of, locale, and goal 4 We Increment the score by 2 points when we have a common semrel and then by another 5 points when the value Is ~dentacal or m the same synset After all possable increments to the scores have been made, we then select the sense(s) w~th the lughest score Finally, we compare our selecuon with that of the gold standard to assess our mapping over all senses Another way an wluch our methodology follows the Senseval process as that at proceeds incrementally Thus, ~t ms not necessary to have a ""final"" perfect parse and mapping rouUne We can make conUnual refinements at any stage of the process and exarmne the overall effect As m Senseval, we may make changes to deal wath a particular phenomenon with the result that overall performance dechnes, but w~th a sounder basis for making subsequent amprovements Results of Componential Analysis The ""gold standard"" analysis Involves mapping 66 WordNet senses with 348 words into 102 Hector senses with 1878 words Using the method described above, we obtained 35 out of 86 correct 3At the present tame, we use WordNet to adentffy semreis We envaslon usmg the full semanlac network created by parsing all a dlcUonary's defimtaons Thas would include a richer set of semrels than currently included m WordNet 4The defimng patterns are developed by hand We have onlyJust begun this effort, so the current set ms somewhat Impoverished mappmgs (407%), a shght improvement over the 31 correct assignments usmg the stop-last word overlap techmque However, as mentioned above, the stophst techmque had aclueved 8 of its successes by matclung null assignments Consadered on tlus basins, ~t seems that the componentaal analysis techmque provides substantial ~mprovement In addition, our technique ""erred"" on 4 cases by malang assagnments where none were made by the leracographer We suggest that these cases do con~n some common elements of meaning and may conceivably not be construed as errors The mapping from WordNet to Hector had relatavely few empty mappings, senses for wtuch It was not possable to make an assignment These are the cases where at appears that the chetmnanes do not overlap and thus prowde a tentative mdacataon of where two dictionaries may have different coverage The cases of multiple assignments mchcate the degree ofamblgmty m the mapping The average m both darecUons between Hector and WordNet were donunated by the mabdaty to obtain good dascnnunatton for the word ""semze"" Thus, tlus method identifies individual words where the &scnnunatwe ablhty needs to be further refined  Perhaps more importantly, the componentml analysis method exploits consaderably more WordNet Hector  mformauon than the word overlap methods Whereas the stop-hst word overlap mapping was  based on only 41 content words, the componenual ~ approach (In the selected mappings) had 228 hits in ~.~  developing ats scores, with only a small number of ~ .~ ~ defining patterns Comparison of Dictionaries tel O ~3 0'3 We next exanuned the nature of the mterrelalaons between parrs of chctaonanes w~thout use of a ""gold standard"" to assess the process of mapping For t/us purpose, we mapped m both &recttons between the paars {WordNet, Hector}, {W3, OALD}, and {W3, AHD We exanune Dorr's lexacal knowledge base for the amphcatlons It may have m the mapping process Neither WordNet nor Hector are properly v~ewed as chcuonanes, since there was no mtenuon to pubhsh them as such WordNet ""glosses"" are generally smaller (53 words per sense) compared to Hector (184 words per sense), whach contains many words specff3nng selectmnal restnct~ons on the subject and object of the verbs Hector was used primarily for a large-scale sense tagging project The three formal d~ctmnanes were subject to rigorous pubhslung and style standards The average number of words per sense were 87 (OALD), 7 1 (AHD), and 9 9 (W3), w~th an average of 3 4, 62, and 120 senses per word Each table shows the average number of senses being mapped, the average number of assignments m the target dlCtmnary, the average number of senses for which no assagnment could be made, the average number of mulUple assignments per word, and the average score of the assignments that were made WN-Hector 37 47 06 17 119 Hector-WN 57 64 14 22 113 These points are further emphasized m the mapping between W3 and OALD, where the disparity between the empty and mulUple assagnments indicate that we are mapping between dictionaries qmte disparate This tends to be the case not only for the enUre set of words, but also is evident for individual words where there is a considerable d~spanty m the number of senses, wtuch then dominate the overall dlspanty Thus, for example, W3 has 41 defimUons for ""float"", while OALD has 10 We tend to be unable to find the specific sense m going from W3 to OALD, because at is likely that we have many more specific defimtlons that are not present In the other direction, we are hkely to have considerable ambiguity and multiple assignments W3-OALD OALD-W3 W3 OALD 120 78 60 18 99 34 60 07 32 86 34 A Between W3 and AHD, there ss less overall daspanty between the defimtaon sets, although since W3 Is tmabndged, we stall have a relatavely lugh number of senses m W3 that do not appear to be present m AHD Finally, It should be noted that the scores for the published dictaonanes tend to be a little lower than for WordNet and Hector Tlus reflects the hkehhood that we have not extracted as much mformataon as we dad m parsing and analyzmg the defimtaon sets used m Senseval W3 AHD oJ  'q O W3-AHD 120 115 40 36 90 AHD-W3 6 2 9 1 1 2 4 1 9 1 We next considered Dorr's lexacal database We first transformed her theta grids to syntactic spectflcataons (transttave or lntransmttve) and identtficataon of semreis (e g, where she Identified an instr component, we added such a semrel to the DIMAP sense) We were able to identify a mappmg from WordNet to her senses for two words (""float"" and ""shake"") for wluch Dorr has several entries However, smce she has considerably more semanuc components than we are currently able to recogmze, we dad not pursue this avenue any further at flus time More important than just mappmg between two words, Dorr's data mdacates the posstbday of further exploitation of a richer set of semanUc components Spectfically, as reported m (Olsen, et al, 1998), m descnbmg procedures for automatically acqumng thematic grids for Mandann Chinese, ~t was noted that ""verbs that incorporate themaUc elements m their meamng would not allow that element to appear m the complement structure"" Thus, by usmg Dorr's thematic grids when verb are parsed m defimtaons, it ~s possible to ~dentffy where partacular semantac components are lexicahzed and which others are transnutted through to the themaUc grid (complement or subcategonzataon pattern) for the defimendum The transmiss~on of semantic components to the thematic gnd ~s also reflected overtly m many defimtlons For example, shake has one definition, ""to bnng to a specified condatton by or as ffby repeated qmck jerky movements"" We would thus expect that the thematac grid for this defimtaon should include a ""goal"" And, deed, Dorr's database has two senses whch reqmre a ""goal"" as part of their thematic grid Smularly, for many defimtaons m the sample set, we ~dentLfied a source defimng pattern based on the word ""from,"" frequently, the object of the preposmon was the word ""source"" ttseff, mdacatmg that the subcategonzaUon, properties of the defimendum should elude a source component Discussion Wlule the improvement m mapping by using the componentaal analysis techmque (over the word overlap methods) is modest, we consider these results qmte slgmficant m wew of the very small number of defimng patterns we have Implemented Most of the improvement stems from the word substatuUon pnnclple described earlier (as ewdenced by the preponderance of 5 point scores) This techmque also provides a mechamsm for bnngmg back the stop words, wz, the preposmons, wluch are the careers of mformatmn about semrels (the 2 point scores) The more general conclusion (from the word subsutuuon) is that the success arises from no longer considenng a defimtmn m ~solation The proper context for a word and its defimtions consists not .lUSt of the words that make up the definition, but also the total semantac network represented by the dictaonary We have aclueved our results by explomng only a small part of that network We have moved only a few steps to that network beyond the mdawdual words and their definitions We would expect that further expansmn, first by the addon of further and ~mproved semrel defining patterns, and second, through the identaficataon of more pnmmve semanuc components, will add considerably to our abflay to map between lexacal resources We also expect ~mprovements from consideration of other techniques, such as attempts at ontology ahgnment (Hovy, 1998) Although tile definition analysis provlded here was performed on definmons with a stogie language, the vanous meamng components m m m m m m m m 35 correspond to those used in an Interhngua The use of the exUncuon method (developed m order to charactenze verbs m another language, Clunese) can frmtfully be applied here as well Two further observaUons about tlus process can be made The first is that rchance on a wellestablished semantic network such as WordNet,s not necessary The componenUal analysis method rehes on the local neighborhood of words m the defimUons, not on the completeness of the network Indeed, the network ~tsel can be bootstrapped based on the parsing results The method can work vath any semanUc network or ontology and may be used to refine or flesh out the network or ontology The second observation is that it is not necessary to have a well-estabhshed ""gold standard"" Any mapping vail do All that Is necessary is for any mvesugator (lemcographer or not) to create a judgmental mappmg The methods employed here can then quanufy ttus mapping based on a word overlap analysis and then further examine tt based on the componenaal analysis The componenUal analysis method can then be used to exanune underlying subtleUes and nuances tn the defimUous, wluch a lemcographer or analyst can then examine m further detail to assess the mapping Future Work Tlus work has marked the first ume that all the necessary mfrastructure has been combmed tn a rudimentary form Because of its rudimentary status, the opportumUes for improvement are quite extensive In addlUon, there are many opportumUes for using the techmques descnbed here m further NLP apphcatlons First, the techmques described here have immediate apphcabtllty as part of a lexicographer's workstaUon When defimUons are parsed and semrels are zdenttfied, the resulUng data structures can be apphed against a corpus of instances for parUcular words (as m Senseval) for improving word-sense disamblguaUon The techmques will also permit comparing an entry vath Itself to deternune the mterrelattonshtps among ~ts defimUons and of companng the defimUons of two ""synonyms"" to deternune the amount of overlap between them on a defimtlon by defimUon bas~s Although the analys,s here has focused on the parsing of defimUous, the development of defimng patterns clearly extends to generalized text parsing since the defimng patterns have been incorporated mto the same chcttonary used for parsing free text, the patterns can be used threctly to identify the presence of parUcular semrels among sentenual consUtuents We are working to integrate th~s funcUonahty into our word-sense &sambiguaUon techruques (both the defimng patterns and the semrels) Even further, mt seems that matclung defimng patterns in free text can be used for lextcal acquisition Textual matenal that contains these patterns could concewably be flagged as providing defimUonal matenal which can then be compared to emstmg defimUons to assess whether their use ts cous,stent vath these defimUons, and ff not, at least to flag the inconsistency The tecluuques descnbed here can be apphed directly to the fields of ontology development and analysis of ternunologlcal databases For ontoiogles, vath or w~thout defimuons, the methods employed can be used to compare entries m dai'erent ontologles based pnmanly on the relattous m the ontology, both luerarclucal and other For ternunologlcal databases, the methods descnbed here can be used to exanune the set of conceptual relaUons lmphed by the defimtmus The defimuon parsing wall facd~tate the development of the termmolog~ca I network tn the pamcular field covered by the database The componenUal analysts methods result m a richer semantic network that can be used m other apphcattous Thus, for example, ~t ts possible to extend the leracal chatmng methods described m (Green, 1997), which are based on the semrels used m WordNet The semrels developed with the componenttal analysis method would provide additional detad available for apphcauon of lexlcal cohesion methods In particular, addtUonal relattous would penmt some structunng wmthm the individual leracal chams, rather than just consldenng each cham as an amorphous set (Green, 1999) Finally, we are currently investigating the use of the componenUal analysts techmque for mformauon extracUon The techmque identifies (from defimtlous) slots that can be used as slots or fields m template generataon Once these slots are identified, we wall be attemptmg to extract slot values from Items m large catalog databases (mdhons of items) 36 In conclusion, it would seem that, instead of a paucity of tnformation allovang us to compare lexmal resources, by bnngmg m the full semantic network of the lexicon, we are overwhelmed with a plethora of data Acknowledgments I would like to thank Bonnie Dorr, Chnstiane Fellbaum, Steve Green, Ed Hovy, Ramesh Knshnamurthy, Bob Krovetz, Thomas Potter, Lucy Vanderwende, and an anonymous reviewer for their comments on an earlier draft of this paper References Atlans, B T S (1991) Bmldmga lexicon The contribution of lexicography lnternattonal Journal of Lextcography, 4(3), 167-204 CL Research (1999a) CL Research Demos  clres com/Demo html CL Research (1999b) Dmtlonary Parsing Project  clres com/dpp html Dolan, W B (1994, 5-9 Aug) Word Sense Amblguation Chistenng Related Senses COLING-94, The 15th International Conference on Computational Linguistics Kyoto, Japan Green, S J (1997) Automatically generating hypertext by computing semantic smulanty \[Dlss\], Toronto, Canada Umverstty of Toronto Green, S J (Sjgreen@mn mq edu au) (1999, 1 June) (Rich semantic networks) Hovy, E (1998, May) Combining and Standardizing Large-Scale, Practical Ontologms for Machine Translation and Other Uses Language Resources and Evaluation Conference Granada, Spam Kalgarnff, A (1998) SENSEVAL Home Page  itn bton ac uk/events/senseval/ Krovetz, R (1992, June) Sense-Linking m a Machine Readable Dictionary 30th Annual Meeting of the Association for Computational Lmgu~stics Newark, Delaware Association for Computational Lmgtustics Lesk, M (1986) Automatic Sense Dlsamblguation Using Machine Readable Dmttonanes How to Tell a Pine Cone from an Ice Cream Cone Proceechngs of SIGDOC Lttkowski, K C (1978) Models of the semantic structure of dictionaries American Journal of Computattonal Lmgutsttcs, Atf 81, 25-74 Lttkowskl, K C (to appear) SENSEVAL The CL Research Expenence Computers and the Humamttes Mtller, G A, Beckwlth, R, Fellbaum, C, Gross, D, & Miller, K J (1990) Introduction to WordNet An on-hne lexical database lnternatwnal Journal of Lexicography, 3(4), 235-244 Olsen, M B, Dorr, B J, & Thomas, S C (1998, 28-31 October) Enhancmg Automatic Acqulsmon of Thematic Structure in a Large-Scale Lexacon for Mandann Chinese Tlurd Conference of the Association for Machine Translation m the Americas, AMTA-98 Langhorne, PA"
P08-1078	P07-1088	o	Recently, some generic methods were proposed to handle context-sensitive inference (Dagan et al., 2006; Pantel et al., 2007; Downey et al., 2007; Connor and Roth, 2007), but these usually treat only a single aspect of context matching (see Section 6).
W06-0113	W95-0107	o	6 Related works After the work of Ramshaw and Marcus (1995), many machine learning techniques have been applied to the basic chunking task, such as Support Vector Machines (Kudo and Matsumoto, 2001), Hidden Markov Model(Molina and Pla 2002), Memory Based Learning (Sang, 2002), Conditional Random Fields (Sha and Pereira, 2003), and so on.
I08-4015	W06-0115	o	Thus, as a powerful sequence tagging model, CRF became the dominant method in the Bakeoff 2006 (Levow, 2006).
P02-1058	J93-1003	o	Proceedings of the 40th Annual Meeting of the Association for In a key step for locating important sentences, NeATS computes the likelihood ratio  (Dunning, 1993) to identify key concepts in unigrams, bigrams, and trigrams1, using the ontopic document collection as the relevant set and the off-topic document collection as the irrelevant set.
P06-1043	J93-2004	o	But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain  the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al. , 1993).
W06-2904	P90-1034	o	For example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990; Pereira et al. , 1993; Grefenstette, 1994; Lin, 1998).
P09-1072	J90-1003	o	4 Using vector-based models of semantic representation to account for the systematic variances in neural activity 4.1 Lexical Semantic Representation Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church and Hanks, 1990).
W00-0737	W95-0107	o	 00: the current input token and the previous one have the same parent  90: one ancestor of the current input token and the previous input token have the same parent  09: the current input token and one ancestor of the previous input token have the same parent  99 one ancestor of the current input token and one ancestor of the previous input token have the same parent Compared with the B-Chunk and I-Chunk used in Ramshaw and Marcus(1995)~, structural relations 99 and 90 correspond to B-Chunk which represents the first word of the chunk, and structural relations 00 and 09 correspond to I-Chunk which represents each other in the chunk while 90 also means the beginning of the sentence and 09 means the end of the sentence.
W02-1011	P02-1053	o	Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002).
P09-1106	P07-1040	o	TER-based: TER-based word alignment method (Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b) is an extension of multiple string matching algorithm based on Levenshtein edit distance (Bangalore et al., 2001).
W06-3603	W96-0213	o	We use the same preprocessing steps as Turian and Melamed (2005): during both training and testing, the parser is given text POS-tagged by the tagger of Ratnaparkhi (1996), with capitalization stripped and outermost punctuation removed.
C04-1060	J97-3002	o	This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in Wu (1997), but the specific bracketing of the parse tree provided.
P06-1059	P05-1045	o	However, other types of nonlocal information have also been shown to be effective (Finkel et al. , 2005) and we will examine the effectiveness of other non-local information which can be embedded into label information.
P07-1093	H05-1087	o	(2006) and Jansche (2005), who discuss maximum expected F-score training of decision trees and logistic regression models.
E06-1032	P02-1040	o	1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al. , 2002).
N07-1007	P03-1021	o	The weights of these models are determined using the max-BLEU method described in Och (2003).
N06-1025	N04-3012	o	1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al. , 2001; Ng & Cardie, 2002; Yang et al. , 2003; Luo et al. , 2004, inter alia).
J99-1004	J96-1002	o	The theory has been applied in probabilistic language modeling (Mark, Miller, and Grenander 1996; Mark et al. 1996; Johnson 1998), natural language processing (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), as well as computational vision (Zhu, Wu, and Mumford 1997).
P07-1055	P04-1035	o	In both cases there 1Alternatively, decisions from the sentence classifier can guide which input is seen by the document level classifier (Pang and Lee, 2004).
C96-1040	J93-2003	p	of the position infer marion of words at ltlat(;hillg pairs of sellte/lCeS, which turned out useful (Brown et al. 1993).
P09-1068	N04-1038	o	In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998).
D07-1027	P04-1041	o	In order to resolve all Chinese NLDs represented in the CTB, we modify and substantially extend the (Cahill et al. , 2004) (henceforth C04 for short) algorithm as follows: Given the set of subcat frames s for the word w, and a set of paths p for the trace t, the algorithm traverses the f-structure f to: predict a dislocated argument t at a sub-fstructure h by comparing the local PRED:w to ws subcat frames s t can be inserted at h if h together with t is complete and coherent relative to subcat frame s traverse f starting from t along the path p link t to its antecedent a if ps ending GF a exists in a sub-f-structure within f; or leave t without an antecedent if an empty path for t exists In the modified algorithm, we condition the probability of NLD path p (including the empty path without an antecedent) on the GF associated of the trace t rather than the antecedent a as in C04.
P09-1099	P02-1040	o	Automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields: widely used measures include BLEU (Papineni et al., 2002) for machine translation and ROUGE (Lin, 2004) for summarisation, for example.
W07-0401	P03-1021	o	Here, we train word alignments in both directions with GIZA++ (Och and Ney, 2003).
P98-2158	J93-2003	n	(Vogel et al. , 1996) report better perplexity results on the Verbmobil Corpus with their HMMbased alignment model in comparison to Model 2 of (Brown et al. , 1993).
P09-3004	P02-1040	o	The measures are: word overlap, length difference (in words), BLEU (Papineni et al., 2002), dependency relation overlap (i.e., R1 and R2 but not FR1,R2), and dependency tree edit distance.
D07-1080	P03-1021	o	2 Statistical Machine Translation We use a log-linear approach (Och, 2003) in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: e = argmax e wT h( f, e) (1) where h( f, e) is a large-dimension feature vector.
W01-0513	J93-1003	o	In particular, we use a randomly-selected corpus the first five columns as information-like. consisting of a 6.7 million word subset of the TREC Similarly, since the last four columns share databases (DARPA, 1993-1997).
P06-1096	P03-1021	o	The first approach is to reuse the components of a generative model, but tune their relative weights in a discriminative fashion (Och and Ney, 2002; Och, 2003; Chiang, 2005).
C08-1038	P02-1040	o	5.2 Experimental Results Following (Langkilde, 2002) and other work on general-purpose generators, BLEU score (Papineni et al., 2002), average NIST simple string accuracy (SSA) and percentage of exactly matched sentences are adopted as evaluation metrics.
J03-4003	W96-0213	o	17 The justification for this is that there is an estimated 3% error rate in the hand-assigned POS tags in the treebank (Ratnaparkhi 1996), and we didnt want this noise to contribute to dependency errors.
C08-1138	P06-1123	o	1 Introduction Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning (Wellington et al., 2006).
W99-0611	J93-2004	o	(1998) present a probabilistic model for pronoun resolution trained on a small subset of the Penn Treebank Wall Street Journal corpus (Marcus et al. , 1993).
C08-1071	P07-1080	o	1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007).
P09-2041	P02-1053	o	3 Method 3.1 Standard text classication approach We take our starting point from topic-based text classication (Dumais et al., 1998; Joachims, 1998) and sentiment classication (Turney, 2002; Pang and Lee, 2008).
W08-0112	J96-2004	o	3 Analysis Results 3.1 Kappa Statistic Kappa coefficient (Carletta, 1996) is commonly used as a standard to reflect inter-annotator agreement.
C00-1023	P95-1026	o	The model can be seen as a bootstrapping learning process tbr disambiguation, where the information gained from one part (selectional preference) is used to improve tile other (disambiguation) and vice versa, reminiscent of the work by Riloff and Jones (1.999) and Yarowsky (1995).
D09-1006	P03-1021	o	Och (2003) shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged.
I05-5008	P02-1040	o	Automatic measures like BLEU (PAPINENI et al. , 2001) or NIST (DODDINGTON, 2002) do so by counting sequences of words in such paraphrases.
W00-1415	J96-2004	o	In other words, (4b) can be used in substitution of (4a), whereas (5b) cannot, so easily 41n (Carletta, 1996), a value of K between .8 and I indicates good agreement; a value between .6 and .8 indicates some agreement.
P08-1023	J97-3002	o	Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006).
C08-1138	J97-3002	o	Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT.
J03-4002	P02-1047	o	In showing how DLTAG and an interpretative process on its derivations operate, we must, of necessity, gloss over how inference triggered by adjacency or associated with a structural connective provides the intended relation between adjacent discourse 578 Computational Linguistics Volume 29, Number 4 units: It may be a matter simply of statistical inference, as in Marcu and Echihabi (2002), or of more complex inference, as in Hobbs et al.
C04-1010	P97-1003	o	Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3), Charniak (2000), and Yamada and Matsumoto (2003).5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of Yamada and Matsumoto (2003).
J05-1003	W02-1001	o	However there has recently been much work drawing connections between the two methods (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002); in this section we review this work.
P08-1108	D07-1013	o	This difference was highlighted in the 3 studyofMcDonaldandNivre(2007), whichshowed that the difference is reflected directly in the error distributions of the parsers.
D09-1050	J93-2003	o	Previous SMT systems (e.g., Brown et al., 1993) used a word-based translation model which assumes that a sentence can be translated into other languages by translating each word into one or more words in the target language.
P06-2018	J96-1002	o	4.2 Cast3LB Function Tagging For the task of Cast3LB function tag assignment we experimented with three generic machine learning algorithms: a memory-based learner (Daelemans and van den Bosch, 2005), a maximum entropy classifier (Berger et al. , 1996) and a Support Vector Machine classifier (Vapnik, 1998).
P93-1003	A92-1018	o	This situation is very similar to that involved in training HMM text taggers, where joint probabilities are computed that a particular word corresponds to a particular part-ofspeech, and the rest of the words in the sentence are also generated (e.g. \[Cutting et al. , 1992\]).
W06-1666	H05-1064	o	1 Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems.
P04-1023	J93-2003	o	1 Introduction Machine translation systems based on probabilistic translation models (Brown et al. , 1993) are generally trained using sentence-aligned parallel corpora.
P98-1010	W95-0107	o	Vilain and Day (1996) identify (and classify) name phrases such as company names, locations, etc. Ramshaw and Marcus (1995) detect noun phrases, by classifying each word as being inside a phrase, outside or on the boundary between phrases.
W07-0719	N03-1017	o	159 2.1 Baseline System The baseline system is a phrase-based SMT system (Koehn et al. , 2003), built almost entirely using freely available components.
W09-1908	N03-1017	n	While the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems (Koehn et al., 2003), the variety of linguistic annotation required is greater.
P09-2086	D07-1090	o	To support distributed computation (Brants et al., 2007), we further split the N-gram data into shards by hash values of the first bigram.
N04-1035	N03-1017	p	Along this line, (Koehn et al. , 2003) present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance  the ability to translate nonconstituent phrases (such as there are, note that, and according to) turns out to be critical and pervasive.
W09-2404	D07-1007	p	In Statistical Machine Translation (SMT), recent work shows that WSD helps translation quality when the WSD system directly uses translation candidates as sense inventories (Carpuat and Wu, 2007; Chan et al., 2007; Gimenez and M`arquez, 2007).
D09-1023	N03-1017	o	2.4 Reordering Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007).
P06-1133	W02-1011	o	There are studies on learning subjective language (Wiebe et al. , 2004), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Riloff et al. , 2003; Riloff and Wiebe, 2003), and discriminating between positive and negative language (Turney and Littman, 2003; Pang et al. , 2002; Dave et al. , 2003; Nasukawa and Yi, 2003; Morinaga et al. , 2002).
D09-1037	N03-1017	n	In contrast, standard phrase-based models (Koehn et al., 2003) assume a mostly monotone mapping between source and target, and therefore cannot adequately model these phenomena.
J02-4005	A00-2024	n	Jing and McKeown (2000) have proposed a rule-based algorithm for sentence combination, but no results have been reported.
W09-2806	W04-1013	o	All submitted runs were evaluated with the automatic metrics: ROUGE (Lin, 2004b), which calculates the proportion of n-grams shared between the candidate summary and the reference summaries, and Basic Elements (Hovy et al., 2005), which compares the candidate to the models in terms of head-modifier pairs.
W07-1520	P06-1097	o	Because of its central role in building machine translation systems and because of the complexity of the task, sub-sentential alignment of parallel corpora continues to be an active area of research (e.g. , Moore et al. , 2006; Fraser and Marcu, 2006), and this implies a continuing demand for manually created or human-verified gold standard alignments for development and evaluation purposes.
H05-2007	W96-0213	o	MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN.
E09-1041	W96-0213	o	(2007), it is much higher than the 2.6% unknown word rate in the test set for Ratnaparkhis (1996) English POS tagging experiments.
J05-4005	P03-1021	o	It is also related to (log-)linear models described in Berger, Della Pietra, and Della Pietra (1996), Xue (2003); Och (2003), and Peng, Feng, and McCallum (2004).
W97-0711	J90-1003	o	of the works of (Kuplec, Pedersen, and Chen, 1995) and (Brandow, Mltze, .and Ran, 1995), and advances summarmatlon technology by applynag corpus-based statistical NLP teehmques, robust information extraction, and readily avaalable on-hne resources Our prehxmnary experiments with combining different summarization features have been reported, and our current effort to learn to combine these features to produce the best summaries has been described The features derived by these robust NLP techmques were also utihzed m presentmg multiple summary.vtews to the user m a novel way References Advanced Research Projects Agency 1995 Proceed:rigs of S:zth Message Understanding Conference (MUC-6) Morgan Kanfmann Pubhshers Brandow, Ron, Karl Mltze, and Lisa Ran 1995 Automatic condensation of electromc pubhcatlous by sentence selection Information Processing and  Management, 31, forthcoming .Bull, Eric 1993 A Comps-based Approach to Language Learning Ph D thesm, Umverslty of Pennsylvania Church, Kenneth and Patrick Hanks 1990 Word  Aesoclatlon Norrns, Mutual Information, and Lexicography Computational Lmgmstscs, 16(1) Church, Kenneth W 1995 One term or two 9 In Proceedings of the 17th Annual International SIGIR Conference on Research and Development In Informatzon Retrzeral, pages 310-318 Edmundson, H P 1969 New methods m automatic abstracting Journal of the ACM, 16(2) 264-228 Fum, Dando, Glovanm Gmda, and Carlo Tasso 1985 Evalutatmg Importance A step towards text surnmarlzatlon In I3CAI85, pages 840-844IJCAi, AAAI Hahn, Udo 1990 Topic parsing Accounting for text macro structures m full-text analysm In format:on Processing and Management, 26(1)135170 Harman, Donna 1991 How effective is suttixang ~ Journal of the Amerlcan Sot:cry for Informatwn Sc:ence, 42(1) 7-15 Harman, Donna 1996 Overview of the fifth text retrieval conference (tree-5) In TREC-5 Conference Proceedings Jmg, Y and B Croft 1994 An Assoc:atwn Thesaurns for Informatzon Retrseval Umass Techmcal Report 94-I7 Center for Intelligent Information Retrieval, University of Massachusetts Johnson, F C, C D Prate, W J Black, and A P Neal 1993.
W06-0306	P02-1053	o	In analyzing opinions (Cardie et al. , 2003; Wilson et al. , 2004), judging document-level subjectivity (Pang et al. , 2002; Turney, 2002), and answering opinion questions (Cardie et al. , 2003; Yu and Hatzivassiloglou, 2003), the output of a sentence-level subjectivity classification can be used without modification.
P08-1058	P07-1065	o	Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al., 2007) no errors are possible for ngrams stored in the model.
W07-0738	J93-2004	o	Tag sets for English are derived from the Penn Treebank (Marcus et al. , 1993).
P09-1103	C08-1138	o	2 We illustrate the rule extraction with an example from the tree-to-tree translation model based on tree sequence alignment (Zhang et al, 2008a) without losing of generality to most syntactic tree based models.
C90-3028	P88-1012	o	It has been implemented in the TACITUS System (Itobbs et al. , 1988, 1990; Stickel, 1989) and has been applied to several varieties of text.
H05-1043	P02-1053	o	Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al. , 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative.
W07-1429	N03-1003	o	Second, we will discuss the work done by (Barzilay & Lee, 2003) who use clustering of paraphrases to induce rewriting rules.
H05-1059	W02-1001	o	Networks (Toutanova et al. , 2003) 97.24 Perceptron (Collins, 2002) 97.11 SVM (Gimenez and Marquez, 2003) 97.05 HMM (Brants, 2000) 96.48 Easiest-first 97.10 Full Bidirectional 97.15 Table 3: POS tagging accuracy on the test set (Sections 22-24 of the WSJ, 5462 sentences).
E09-1033	N06-1020	o	Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al., 2006).
W00-0103	C94-2113	p	This approach took inspiration from the pioneering work by (Dolan 1994), but it is also fundamentally different, because instead of grouping similar senses together, the CoreLex approach groups together words according to all of their senses.
D08-1050	J93-2004	o	1 Introduction Most state-of-the-art wide-coverage parsers are based on the Penn Treebank (Marcus et al., 1993), making such parsers highly tuned to newspaper text.
P03-1030	J93-1003	o	Our approach was to identify a parallel corpus of manually and automatically transcribed documents, the TDT2 corpus, and then use a statistical approach (Dunning, 1993) to identify tokens with significantly Table 5: Impact of recall and precision enhancing devices.
C08-1051	C98-2122	o	 Three K-means algorithms using different distributional similarity or dissimilarity measures: cosine, -skew divergence (Lee, 1999) 4 , and Lins similarity (Lin, 1998).
H05-1058	W96-0213	o	This is based on the idea from (Ratnaparkhi, 1996) that rare words in the training set are similar to unknown words in the test set, and can be used to learn how to tag the unknown words that will be encountered during testing.
D09-1116	J92-4003	o	Models of this type include: (Brown et al., 1992; Zitouni, 2007), which use semantic word clustering, and (Bahl et al., 1990), which uses variablelength context.
C08-1009	C98-2122	o	As described in Section 3 we retrieved neighbors using Lins (1998) similarity measure on a RASP parsed (Briscoe and Carroll, 2002) version of the BNC.
W09-2310	N03-1017	p	The state-of-the-art SMT system Moses implements a distance-based reordering model (Koehn et al., 2003) and a distortion model, operating with rewrite patterns extracted from a phrase alignment table (Tillman, 2004).
W03-0407	W96-0213	o	3ThePOS taggers The two POS taggers used in the experiments are TNT, a publicly available Markov model tagger (Brants, 2000), and a reimplementation of the maximum entropy (ME) tagger MXPOST (Ratnaparkhi, 1996).
D07-1115	P02-1053	o	This idea is the same as (Turney, 2002).
W07-0715	P02-1040	o	Since this trade-off is also affected by the settings of various pruning parameters, we compared decoding time and translation quality, as measured by BLEU score (Papineni et al, 2002), for the two models on our first test set over a broad range of settings for the decoder pruning parameters.
N07-1046	J96-1002	o	With hand-labeled data, {m} can be learnt via generalized iterative scaling algorithm (GIS) (Darroch and Ratcliff, 1972) or improved iterative scaling (IIS) (Berger 367 et al. , 1996).
P09-1104	J97-3002	o	Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting.
P06-2113	W02-1001	o	3.3 CRFs and Perceptron Learning Perceptron training for conditional models (Collins, 2002) is an approximation to the SGD algorithm, using feature counts from the Viterbi label sequence in lieu of expected feature counts.
P07-1026	J93-2004	o	The data consists of sections of the Wall Street Journal part of the Penn TreeBank (Marcus et al. , 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al. , 2005).
N07-2022	J93-2003	o	1 Introduction In the first SMT systems (Brown et al. , 1993), word alignment was introduced as a hidden variable of the translation model.
W04-0701	J96-1002	o	models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence, but otherwise is as uniform as possible (Berger et al. , 1996).
W04-2407	J93-2004	o	Thus, the Penn Treebank of American English (Marcus et al. , 1993) has been used to train and evaluate the best available parsers of unrestricted English text (Collins, 1999; Charniak, 2000).
W01-1605	J93-2004	o	 Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Hovy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein et al. , 2001).
P06-1089	J96-1002	o	p0(t|w) is calculated by ME models as follows (Berger et al. , 1996): p0(t|w)= 1Y(w) exp braceleftBigg Hsummationdisplay h=1 hgh(w,t) bracerightBigg, (20) 709 Language Features English Prefixes of 0 up to four characters, suffixes of 0 up to four characters, 0 contains Arabic numerals, 0 contains uppercase characters, 0 contains hyphens.
W09-1804	D07-1006	o	Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f | e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997).
C04-1072	P03-1021	o	To simulate real world scenario, we use n-best lists from ISIs state-of-the-art statistical machine translation system, AlTemp (Och 2003), and the 2002 NIST Chinese-English evaluation corpus as the test corpus.
P04-1006	J93-2004	o	The first stage parser is a best-first PCFG parser trained on sections 2 through 22, and 24 of the Penn WSJ treebank (Marcus et al. , 1993).
C92-1033	J90-1003	o	a Hindle and Rooth (1991) and Church and Hanks (1990) used partial parses generated by Fidditch to study word ~urrt.nc patterns m syntactic contexts.
N09-1049	P07-1019	p	433 Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed.
W08-2124	W02-1001	o	It combines online Peceptron learning (Collins, 2002) with a parsing model based on the Eisner algorithm (Eisner, 1996), extended so as to jointly assign syntactic and semantic labels.
W05-1514	W95-0107	o	Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus.
D09-1073	J97-3002	p	1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT).
D09-1120	P05-1045	o	Instead, we opt to utilize the Stanford NER tagger (Finkel et al., 2005) over the sentences in a document and annotate each NP with the NER label assigned to that mention head.
W00-0707	J96-1002	o	For a given choice of q and f, the IIS algorithm (Berger et al. , 1996) can be used to find maximum likelihood values for the parameters ~.
H05-1019	W04-1013	o	For ROUGE-S and ROUGE-SU, we use three variations following (Lin, 2004b): the maximum skip distances are 4, 9 and infinity 7.
P97-1037	J93-2003	o	Models describing these types of dependencies are referred to as alignrnen.t models (Brown et al. , 1993), (Dagan eta\] 1993).
P06-1065	W02-1001	o	The principal training method is an adaptation of averaged perceptron learning as described by Collins (2002).
P99-1009	J93-2004	o	1 To train their system, R&M used a 200k-word chunk of the Penn Treebank Parsed Wall Street Journal (Marcus et al. , 1993) tagged using a transformation-based tagger (Brill, 1995) and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics (like treating the possessive marker as the first word of a new base noun phrase) to flatten the recursive structure of the parse.
P05-1058	J93-1003	o	2 Statistical Word Alignment According to the IBM models (Brown et al. , 1993), the statistical word alignment model can be generally represented as in Equation (1).
E95-1020	A92-1018	o	We obtained 47,025 50-dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot (Cutting et al. , 1992) (group average agglomeration applied to a sample).
H05-1117	P02-1040	p	3 Previous Work The idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the BLEU metric for machine translation (Papineni et al. , 2002).
P00-1061	J93-2004	o	In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al. , 1993) is used, the availability of whichobviates the standard eort required for treebank traininghandannotating large corpora of specic domains of specic languages with specic parse types.
P06-2004	J93-2004	o	With the exception of (Hindle and Rooth, 1993), most unsupervised work on PP attachment is based on superficial analysis of the unlabeled corpus without the use of partial parsing (Volk, 2001; Calvo et al. , 2005).
C08-1014	P07-1040	o	Re-decoding (Rosti et al., 2007a) based regeneration re-decodes the source sentence using original LM as well as new trans105 lation and reordering models that are trained on the source-to-target N-best translations generated in the first pass.
P09-1103	N03-1017	o	Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation.
P07-1055	W02-1001	p	Weight averaging was also employed (Collins, 2002), which helped improve performance.
D09-1136	J93-2003	o	Becausesuchapproachesdirectly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996).
P07-1091	P02-1040	o	(Case-sensitive) BLEU-4 (Papineni et al. , 2002) is used as the evaluation metric.
P04-3020	C04-1162	o	Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al. , 2004).
W03-0806	W96-0213	o	For instance, implementing an efficient version of the MXPOST POS tagger (Ratnaparkhi, 1996) will simply involve composing and configuring the appropriate text file reading component, with the sequential tagging component, the collection of feature extraction components and the maximum entropy model component.
W03-0604	J93-2003	o	We have developed a set of extensions to a probabilistic translation model (Brown et al. , 1993) that enable us to successfully merge oversegmented regions into coherent objects.
W09-2808	A00-2024	o	Like the work of Jing and McKeown (2000) and Mani et al.
D08-1059	P07-1050	o	An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007).
W95-0106	J93-2003	o	"It is interesting to constrast this method with the ""parse-parse-match"" approaches that have been reported recently for producing parallel bracketed corpora (Sadler & Vendelmans 1990; Kaji et al. 1992; Matsumoto et al. 1993; Cranias et al. 1994; Gfishman 1994)."
C08-1041	P02-1040	o	The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with case-insensitive matching of n-grams, where n =4.
W08-2130	J96-1002	o	In this paper a discriminative parser is proposed to implement maximum entropy (ME) models (Berger, et al., 1996) to address the learning task.
J04-4004	P97-1003	o	503 Bikel Intricacies of Collins Parsing Model Table 4 Overall parsing results using only details found in Collins (1997, 1999).
H05-1021	P03-1021	o	Finally we use Minimum Error Training (MET) (Och, 2003) to train log-linear scaling factors that are applied to the WFSTs in Equation 1.
J01-1004	P97-1003	o	recent advances in parsing technology are due to the explicit stochastic modeling of dependency information (Collins 1997).
N09-1013	J96-1002	o	Since it is not feasible to maximise the likelihood of the observations directly, we maximise the expected log likelihood by considering the EM auxiliary function, in a similar manner to that used for modelling contextual variations of phones for ASR (Young et al., 1994; Singer and Ostendorf, 1996).
W09-2303	P06-1123	o	It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006).
W05-0816	J93-2003	o	Word correspondence was further developed in IBM Model-1 (Brown et al. , 1993) for statistical machine translation.
H05-1099	W95-0107	o	2 Evaluating Heterogeneous Parser Output Two commonly reported shallow parsing tasks are Noun-Phrase (NP) Chunking (Ramshaw and Marcus, 1995) and the CoNLL-2000 Chunking task (Sang and Buchholz, 2000), which extends the NPChunking task to recognition of 11 phrase types1 annotated in the Penn Treebank.
W04-2203	P02-1040	o	3.1 Golden-standard-based criteria In the domain of machine translation systems, an increasingly accepted way to measure the quality of a system is to compare the outputs it produces with a set of reference translations, considered as an approximation of a golden standard (Papineni et al. , 2002; hovy et al. , 2002).
J00-2004	J93-1003	o	The performance of cross-language information retrieval with a uniform T is likely to be limited in the same way as the performance of conventional information retrieval without term-frequency information, i.e., where the system knows which terms occur in which documents, but not how often (Buckley 1993).
J07-4005	J05-4002	o	Further, it has been shown (Weeds et al. 2005; Weeds and Weir 2005) that performance of Lins distributional similarity score decreases more significantly than other measures for low frequency nouns.
D09-1092	J93-2003	o	In the early statistical translation model work at IBM, these representations were called cepts, short for concepts (Brown et al., 1993).
P01-1050	J93-2003	o	In this framework, the source language, let-s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al. , 1993).
E06-1019	J93-2003	o	The task originally emerged as an intermediate result of training the IBM translation models (Brown et al. , 1993).
P97-1041	W95-0107	o	Transformation-based learning has also been successfully applied to text chunking (Ramshaw and Marcus, 1995), morphological disambiguation (Oflazer and Tur, 1996), and phrase parsing (Vilain and Day, 1996).
D08-1109	N06-1041	o	In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006).
P08-1024	P06-1096	n	Both the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets.
P98-2243	J90-1003	o	The cohesion between words has been evaluated with the mutual information measure, as in (Church and Hanks, 1990).
W99-0628	J93-2004	o	A very impor232 Author Best Hindle and Rooth (1993) 80.0 % Resnik and Hearst (1993) 83.9 % WN Resnik and Hearst (1993) 75.0 % Ratnaparkhi et al.
P04-1052	J93-2004	o	4 Evaluation As our algorithm works in open domains, we were able to perform a corpus-based evaluation using the Penn WSJ Treebank (Marcus et al. , 1993).
D09-1037	N03-1017	o	These heuristics are extensions of those developed for phrase-based models (Koehn et al., 2003), and involve symmetrising two directional word alignments followed by a projection step which uses the alignments to find a mapping between source words and nodes in the target parse trees (Galley et al., 2004).
N04-1020	P02-1047	o	A similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi (2002).
P06-1046	J05-4002	o	This is important when LARGE CUT-OFF 0 5 100 NAIVE 541,721 184,493 35,617 SASH 10,599 8,796 6,231 INDEX 5,844 13,187 32,663 Table 4: Average number of comparisons per term considering that different tasks may require different weights and measures (Weeds and Weir, 2005).
W07-0704	N03-1017	o	We employ the phrase-based SMT framework (Koehn et al. , 2003), and use the Moses toolkit (Koehn et al. , 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al. , 2002), using a single reference translation.
N07-1022	P02-1040	o	#Reference: If our player 2, 3, 7 or 5 has the ball and the ball is close to our goal line  PHARAOH++: If player 3 has the ball is in 2 5 the ball is in the area near our goal line  WASP1++: If players 2, 3, 7 and 5 has the ball and the ball is near our goal line  Figure 4: Sample partial system output in the ROBOCUP domain ROBOCUP GEOQUERY BLEU NIST BLEU NIST PHARAOH 0.3247 5.0263 0.2070 3.1478 WASP1 0.4357 5.4486 0.4582 5.9900 PHARAOH++ 0.4336 5.9185 0.5354 6.3637 WASP1++ 0.6022 6.8976 0.5370 6.4808 Table 1: Results of automatic evaluation; bold type indicates the best performing system (or systems) for a given domain-metric pair (p < 0.05) 5.1 Automatic Evaluation Weperformed4runsof10-foldcrossvalidation,and measured the performance of the learned generators using the BLEU score (Papineni et al. , 2002) and the NIST score (Doddington, 2002).
D09-1079	P08-1058	o	As with other randomised models we construct queries with the appropriate sanity checks to lower the error rate efficiently (Talbot and Brants, 2008).
W08-0302	N03-1017	o	(2003), in which we translate a source-language sentence f into the target-language sentence e that maximizes a linear combination of features and weights:1 e,a = argmax e,a score(e,a,f) (1) = argmax e,a Msummationdisplay m=1 mhm(e,a,f) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight m. The translation is typically found using beam search (Koehn et al., 2003).
C08-1114	J06-3003	n	 The automatically generated patterns in PairClass are slightly more general than the patterns of Turney (2006).
D07-1112	J93-2004	o	The annotation guidelines for the Penn Treebank flattened noun phrases to simplify annotation (Marcus et al. , 1993), so there is no complex structure to NPs.
N09-2054	P05-1010	o	Building upon the large body of research to improve tagging performance for various languages using various models (e.g., (Thede and Harper, 1999; Brants, 2000; Tseng et al., 2005b; Huang et al., 2007)) and the recent work on PCFG grammars with latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006), we will investigate the use of fine-grained latent annotations for Chinese POS tagging.
D09-1070	P95-1026	o	Our intuition comes from an observation by Yarowsky (1995) regarding multiple tokens of words in documents.
A92-1013	P90-1034	o	Combining statistical and parsing methods has been done by (Hindle, 1990; Hindle and Rooths,1991) and (Smadja and McKewon, 1990; Smadja,1991).
D07-1068	N06-1025	o	Strube and Ponzetto explored the use of Wikipedia for measuring Semantic Relatedness between two concepts (2006), and for Coreference Resolution (2006).
P06-2111	J93-2003	p	For the word alignment, we apply standard techniques derived from statistical machine translation using the well-known IBM alignment models (Brown et al. , 1993) implemented in the opensource tool GIZA++ (Och, 2003).
E06-1011	P04-1015	o	Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment (Moore, 2005), sequence analysis (Daume and Marcu, 2005; McDonald et al. , 2005a) and phrase-structure parsing (Collins and Roark, 2004).
P09-1028	W02-1011	o	Most work in machine learning literature on utilizing labeled features has focused on using them to generate weakly labeled examples that are then used for standard supervised learning: (Schapire et al., 2002) propose one such framework for boosting logistic regression; (Wu and Srihari, 2004) build a modi ed SVM and (Liu et al., 2004) use a combination of clustering and EM based methods to instantiate similar frameworks.
P08-1011	P02-1040	o	In addition to precision and recall, we also evaluate the Bleu score (Papineni et al., 2002) changes before and after applying our measure word generation method to the SMT output.
W05-1516	P90-1034	o	For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct,  and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult,  Many methods have been proposed to compute distributional similarity between words (Hindle, 1990; Pereira et al. , 1993; Grefenstette, 1994; Lin, 1998).
W09-0439	P03-1021	p	Ochs procedure is the most widely-used version of MERT for SMT (Och, 2003).
D08-1023	P03-1021	o	Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007).
H05-1061	J93-2003	p	One widely used model is the IBM model (Brown et al. 1993).
P03-2006	J93-2004	o	The corpus was automatically derived from the Penn Treebank II corpus (Marcus et al. , 1993), by means of the script chunklink.pl (Buchholz, 2002) that we modified to fit our purposes.
W07-1431	P85-1008	o	Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985).
C08-1051	C98-2122	o	Others proposed distributional similarity measures between words (Hindle, 1990; Lin, 1998; Lee, 1999; Weeds et al., 2004).
P06-1073	J96-1002	o	Our appoach is based on Maximum Entropy (MaxEnt henceforth) technique (Berger et al. , 1996).
C08-1042	D07-1031	o	Following the setup in Johnson (2007), we initialize the transition and emission distributions to be uniform with a small amount of noise, and run EM and VB for 1000 iterations.
E06-1032	P03-1021	p	The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003).
W09-2306	N03-1017	p	1 Introduction Phrase-based statistical machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model.
E09-3008	P02-1040	o	This system was worse than the baseline on Bleu (Papineni et al., 2002), but an error analysis showed some improvements.
I08-2087	P03-1021	o	(2003), bilingual sentences are trained by GIZA++ (Och and Ney 2003) in two directions (from source to target and target to source).
W08-0304	P03-1021	o	This is seen in that each time we check for the nearest intersection to the current 1-best for some n-best list l, we Algorithm 1 Och (2003)s line search method to find the global minimum in the loss, lscript, when starting at the point w and searching along the direction d using the candidate translations given in the collection of n-best lists L. Input: L, w, d, lscript I {} for l L do for e  l do m{e} e.features d b{e} e.features w end for bestn argmaxel m{e}{b{e} breaks ties} loop bestn+1 = argminel max parenleftBig 0, b{bestn}b{e}m{e}m{bestn} parenrightBig intercept  max parenleftBig 0, b{bestn}b{bestn+1}m{bestn+1}m{bestn} parenrightBig if intercept > 0 then add(I, intercept) else break end if end loop end for add(I, max(I)+2epsilon1) ibest = argminiI evallscript(L,w+(iepsilon1)d) return w+(ibest epsilon1)d must calculate its intersection with all other candidate translations that have yet to be selected as the 1-best.
W03-1903	J96-2004	o	In this sense, instead of measuring only the categorial agreement between annotators with the kappa statistic (Carletta, 1996) or the performance of a system in terms of precision/recall, we could take into account the hierarchical organization of the categories or concepts by making use of measures considering the hierarchical distance between two concepts such as proposed by (Hahn and Schnattinger, 1998) or (Madche et al. , 2002).
P06-2112	J97-3002	o	Many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Zhang and Gildea, 2005).
D09-1066	C02-1007	o	Then, by using evaluations similar to those described in (Baroni et al., 2008) and by Rapp (2002), we show that the best distance-based measures correlate better overall with human association scores than do the best window based configurations (see Section 4), and that they also serve as better predictors of the strongest human associations (see Section 5).
P07-1090	J97-3002	o	Instead of using Inversion Transduction Grammar (ITG) (Wu, 1997) directly, we will discuss an ITG extension to accommodate gapping.
P08-1067	N06-1020	o	type system F1% D Collins (2000) 89.7 Henderson (2004) 90.1 Charniak and Johnson (2005) 91.0 updated (Johnson, 2006) 91.4 this work 91.7 G Bod (2003) 90.7Petrov and Klein (2007) 90.1 S McClosky et al.
P09-1034	P03-1021	o	We are currently investigating caching and optimizations that will enable the use of our metric for MT parameter tuning in a Minimum Error Rate Training setup (Och, 2003).
J05-4003	P03-1021	o	All our MT systems were trained using a variant of the alignment template model described in (Och 2003).
W04-2105	J93-1003	o	The statistical significance often evaluate whether two words are independant using hypothesis tests such as t-score (Church et al. , 1991), the X2, the log-likelihood (Dunning, 1993) and Fishers exact test (Pedersen, 1996).
P09-1036	P03-1021	o	These constituent matching/violation counts are used as a feature in the decoders log-linear model and their weights are tuned via minimal error rate training (MERT) (Och, 2003).
N09-1051	J92-4003	o	(4) can be used to motivate a novel class-based language model and a regularized version of minimum discrimination information (MDI) models (Della Pietra et al., 1992).
W99-0631	J93-1003	o	We use the log-likelihood X ~ statistic, rather than the Pearson's X 2 statistic, as this is thought to be more appropriate when the counts in the contingency table are low (Dunning, 1993).
P08-2065	W02-1011	o	1 Introduction Sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of, or sentiment toward a given subject (e.g., if an opinion is supported or not) (Pang et al., 2002).
P06-2070	W05-0909	o	In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005).
W05-1514	J96-1002	o	6 Phrase Recognition with a Maximum Entropy Classifier For the candidates which are not filtered out in the above two phases, we perform classification with maximum entropy classifiers (Berger et al. , 1996).
P03-2006	J93-2004	o	We performed experiments with two statistical classifiers: the decision tree induction system C4.5 (Quinlan, 1993) and the Tilburg Memory-Based Learner (TiMBL) (Daelemans et al. , 2002).
P06-1121	N04-1035	n	Finally, we show that our contextually richer rules provide a 3.63 BLEU point increase over those of (Galley et al. , 2004).
W03-0505	J93-2004	o	Table 3 compares precision, recall, and F scores for our system with CoNLL-2001 results training on sections 15-18 of the Penn Treebank and testing on section 21 (Marcus et al. , 1993).
W06-1670	H05-1064	p	In syntactic parse re-ranking supersenses have been used to build useful latent semantic features (Koo and Collins, 2005).
W03-0416	J92-4003	o	Examples have been class-based D2-gram models (Brown et al. , 1992; Kneser and Ney, 1993), smoothing techniques for structural disambiguation (Li and Abe, 1998) and word sense disambiguation (Shutze, 1998).
W06-0305	J93-2004	o	2 The Penn Discourse TreeBank (PDTB) The PDTB contains annotations of discourse relations and their arguments on the Wall Street Journal corpus (Marcus et al. , 1993).
D08-1069	P07-1107	o	This therefore suggests that better parameters are likely to be learned in the 2Haghighi and Kleins (2007) generative coreference model mirrors this in the posterior distribution which it assigns to mention types given their salience (see their Table 1).
I08-2119	N07-1015	o	Feature-based methods (Jiang and Zhai, 2007; Kambhatla, 2004; Zhou et al., 2005) use pre-defined feature sets to extract features to train classification models.
P09-1041	D07-1014	n	Unfortunately, there is no straightforward generalization of the method of Smith and Smith (2007) to the two edge marginal problem.
W09-1119	W02-1001	o	NER is typically viewed as a sequential prediction problem, the typical models include HMM (Rabiner, 1989), CRF (Lafferty et al., 2001), and sequential application of Perceptron or Winnow (Collins, 2002).
N09-1014	N07-1026	o	In natural language processing, label propagation has been used for document classification (Zhu, 2005), word sense disambiguation (Niu et al., 2005; Alexandrescu and Kirchhoff, 2007), and sentiment categorization (Goldberg and Zhu, 2006).
E99-1023	W95-0107	p	"(l~mshaw and Marcus, 1995) have introduced a ""convenient"" data representation for chunking by converting it to a tagging task."
C08-1103	W06-0301	p	A notable exception is the work of Kim and Hovy (2006).
D09-1018	W06-1642	o	2 Related Work Previous work on polarity disambiguation has used contextual clues and reversal words (Wilson et al., 2005; Kennedy and Inkpen, 2006; Kanayama and Nasukawa, 2006; Devitt and Ahmad, 2007; Sadamitsu et al., 2008).
D09-1106	N03-1017	o	The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case.
W09-0104	J93-2004	o	I have made a preliminary analysis of the inventory of syntactic categories used in the tagging for labelling trees in the 18 Penn Treebank (Marcus et al., 1993), comparing them to the categories used in CGEL.
W05-0828	P02-1040	o	3.2 Results and Discussion The BLEU scores (Papineni et al. , 2002) for 10 direct translations and 4 sets of heuristic selections 4Admittedly, in typical instances of such chains, English would appear earlier.
W08-0308	P07-1003	n	DeNero and Klein (2007) focus on alignment and do not present MT results, while May and Knight (2007) takesthesyntacticre-alignmentasaninputtoanEM algorithm where the unaligned target words are insertedintothetemplatesandminimumtemplatesare combinedintobiggertemplates(Galleyetal.,2006).
W09-0424	P03-1021	o	Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objective is to minimize the onebest error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique).
W04-2208	J93-2004	p	On the other hand, high-quality treebanks such as the Penn Treebank (Marcus et al. , 1993) and the Kyoto University text corpus (Kurohashi and Nagao, 1997) have contributed to improving the accuracies of fundamental techniques for natural language processing such as morphological analysis and syntactic structure analysis.
D09-1008	P06-1091	o	One is to use a stochastic gradient descent (SGD) or Perceptron like online learning algorithm to optimize the weights of these features directly for MT (Shen et al., 2004; Liang et al., 2006; Tillmann and Zhang, 2006).
W09-2303	J97-3002	o	of Linguistics University of Potsdam kuhn@ling.uni-potsdam.de Abstract The empirical adequacy of synchronous context-free grammars of rank two (2-SCFGs) (Satta and Peserico, 2005), used in syntaxbased machine translation systems such as Wu (1997), Zhang et al.
J04-3001	P97-1003	o	Moreover, in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain, we have performed the study on two parsing models: one based on a context-free variant of tree-adjoining grammars (Joshi, Levy, and Takahashi 1975), the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters 1993; Hwa 1998), and Collinss Model 2 parser (1997).
J98-1003	P95-1026	o	Using thesaurus categories directly as a coarse sense division may seem to be a viable alternative (Yarowsky 1995).
D09-1156	P06-1101	o	1510 5 Related Work In recent years, many research has been done on extracting relations from free text (e.g., (Pantel and Pennacchiotti, 2006; Agichtein and Gravano, 2000; Snow et al., 2006)); however, almost all of them require some language-dependent parsers or taggers for English, which restrict the language of their extractions to English only (or languages that have these parsers).
W08-1808	W04-1013	n	We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task.
W04-2803	P85-1008	o	After the parser produces a semantic feature structure representation of the sentence, predicate mapping rules then match against that representation in order to produce a predicate language representation in the style of Davidsonian event based semantics (Davidson, 1967; Hobbs, 1985), as mentioned above.
E06-1046	J93-2003	o	We adopt an approach, similar to (Ciaramella, 1993; Boros et al. , 1996), in which the meaning representation, in our case XML, is transformed into a sorted flat list of attribute-value pairs indicating the core contentful concepts of each command.
C08-1107	C98-2122	o	Texts are represented by dependency parse trees (using the Minipar parser (Lin, 1998b)) and templates by parse sub-trees.
N03-1017	P97-1003	o	To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000].
N09-1013	J93-2003	o	Standard CI Model 1 training, initialised with a uniform translation table so that t(ejf) is constant for all source/target word pairs (f,e), was run on untagged data for 10 iterations in each direction (Brown et al., 1993; Deng and Byrne, 2005b).
C04-1111	P90-1034	o	2.2 Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics (Hindle 1990, Lin 1998).
P07-1078	H05-1064	o	A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features.
D08-1058	P04-1035	o	(2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005).
P09-1034	W05-0904	o	(2008) to LFG parses, and by Liu and Gildea (2005) to features derived from phrase-structure tress.
J93-2005	P90-1034	o	Hindle (1990) reports interesting results of this kind based on literal collocations, where he parses the corpus (Hindle 1983) into predicate-argument structures and applies a mutual information measure (Fano 1961; Magerman and Marcus 1990) to weigh the association between the predicate and each of its arguments.
P06-2034	W02-1001	o	While reranking has benefited many tagging and parsing tasks (Collins, 2000; Collins, 2002c; Charniak and Johnson, 2005) including semantic role labeling (Toutanova et al. , 2005), it has not yet been applied to semantic parsing.
P08-1010	N03-1017	p	The most widely used approach derives phrase pairs from word alignment matrix (Och and Ney, 2003; Koehn et al., 2003).
P04-1013	W02-1002	n	While both (Johnson, 2001) and (Klein and Manning, 2002) propose models which use the parameters of the generative model but train to optimize a discriminative criteria, neither proposes training algorithms which are computationally tractable enough to be used for broad coverage parsing.
D09-1021	N03-1017	o	The method thereby retains the full set of lexical entries of phrase-based systems (e.g., (Koehn et al., 2003)).1  The model allows a straightforward integration of lexicalized syntactic language modelsfor example the models of (Charniak, 2001)in addition to a surface language model.
P05-1049	P95-1026	o	Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al. , 1998), semi-supervised learning algorithms (Yarowsky, 1995), and unsupervised learning algorithms (Schutze, 1998).
E95-1020	A92-1018	o	No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al. , 1991; Kupiec, 1992).
W09-2420	P07-1033	o	1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007).
W05-1609	P85-1008	o	2 Background 2.1 Hybrid Logic Dependency Semantics Hybrid Logic Dependency Semantics (HLDS; [Kruijff, 2001; Baldridge and Kruijff, 2002]) is an ontologically promiscuous [Hobbs, 1985] framework for representing the propositional content (or meaning) of an expression as an ontologically richly sorted, relational structure.
W05-0836	P03-1021	o	In the following, we summarize the optimization algorithm for the unsmoothed error counts presented in (Och, 2003) and the implementation detailed in (Venugopal and Vogel, 2005).
W09-2603	J93-2004	p	The Penn Treebank (Marcus et al., 1993) has until recently been the only such corpus, covering 4.5M words in a single genre of financial reporting.
J98-1003	C94-2113	o	82 Chen and Chang Topical Clustering Dolan (1994) maintains the position that intersense relations are mostly idiosyncratical, thereby making it difficult to characterize them in a general way so as to identify them.
D07-1030	J93-2003	o	SMT has evolved from the original word-based approach (Brown et al. , 1993) into phrase-based approaches (Koehn et al. , 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al. , 2000; Yamada and Knignt, 2001; Chiang, 2005).
W01-1004	J92-4003	o	In the literature approaches to construction of taxonomies of concepts have been proposed (Brown et al. 1992, McMahon and Smith 1996, Sanderson and Croft 1999).
W09-1008	P05-1010	o	This leads to 49 methods that use semi-supervised techniques on a treebank-infered grammar backbone, such as (Matsuzaki et al., 2005; Petrov et al., 2006).
N06-2004	N04-3012	o	We use the default configuration of the measure in WordNet::Similarity-0.12 package (Pedersen et al. , 2004), and, with a single exception, the measure performed below Gic; see BP in table 1.
H05-1098	N03-1017	o	The basic model uses the following features, analogous to Pharaohs default feature set:  P( | ) and P( | )  the lexical weights Pw( | ) and Pw( | ) (Koehn et al. , 2003);1  a phrase penalty exp(1);  a word penalty exp(l), where l is the number of terminals in .
P09-1065	P03-1021	o	While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account.
P02-1044	P95-1026	o	Yarowsky (1995) proposes a method for word sense disambiguation, which is based on Monolingual Bootstrapping.
P05-1012	W96-0213	o	Our system assumes POS tags as input and uses the tagger of Ratnaparkhi (1996) to provide tags for the development and evaluation sets.
W06-1670	W02-1001	o	To this extent, we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model (HMM), based on that of Collins (2002), on the manually annotated Semcor corpus (Miller et al. , 1993).
W08-2122	P07-1080	o	P(ctd|C1,,Ct1) =producttextiP(Did|Dbtdd ,,Di1d ,C1,,Ct1) (3) The actions are also sometimes split into a sequence of elementary decisions Di = di1,,din, as discussed in (Titov and Henderson, 2007a).
H05-1091	P97-1003	o	In our experiments, we used the full parse output from Collins parser (Collins, 1997), in which every non-terminal node is already annotated with head information.
D07-1096	D07-1013	o	We then describe the two main paradigms for learning and inference, in this years shared task as well as in last years, which we call transition-based parsers (section 5.2) and graph-based parsers (section 5.3), adopting the terminology of McDonald and Nivre (2007).5 Finally, we give an overview of the domain adaptation methods that were used (section 5.4).
D07-1023	J93-2004	o	We use as our English corpus the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al. , 1993).
W08-0305	P03-1021	o	These models can be tuned using minimum error rate training (Och, 2003).
P09-1067	P03-1021	o	In the geometric interpolation above, the weight n controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006).
E09-1011	W96-0213	o	We then proceed to split the data into smaller sentences and tag them using Ratnaparkhis Maximum Entropy Tagger (Ratnaparkhi, 1996).
P98-2127	P90-1034	o	The measure simHinate is the same as the similarity measure proposed in (Hindle, 1990), except that it does not use dependency triples with negative mutual information.
E06-1018	J93-1003	o	The significance values are obtained using the loglikelihood measure assuming a binomial distribution for the unrelatedness hypothesis (Dunning, 1993).
W04-2602	J92-4003	o	Our approach to inducing syntactic clusters is closely related to that described in Brown, et al, (1992) which is one of the earliest papers on the subject.
W05-0815	J97-3002	o	This model shares some similarities with the stochastic inversion transduction grammars (SITG) presented by Wu in (Wu, 1997).
H05-1108	P97-1003	o	For the constituent-based models, constituent information was obtained from the output of Collins parser (1997) for English and Dubeys parser (2004) for German.
D07-1033	W02-1001	o	This resembles the re-ranking approach (Collins and Duffy, 2002; Collins, 2002b).
W03-1025	J96-1002	p	Each component model takes the exponential form: a37a55a38a57a56 a51 a42a6a44a59a58a60a56 a61 a51a64a63a65a53a67a66 a53 a45a46a70 a71a16a72a21a73a75a74a77a76a79a78a81a80 a78a16a82a11a78 a38a83a44a59a58a60a56a84a61 a51a64a63a65a53a67a66 a53 a58a60a56 a51 a45a86a85 a87 a38a83a44a59a58a60a56a84a61 a51a64a63a65a53a67a66 a53 a45 a58 (2) where a87 a38a83a44a59a58a60a56 a61 a51a41a63a65a53a67a66 a53 a45 is a normalization term to ensure that a37a55a38a57a56 a51a42a6a44a88a58a60a56a62a61 a51a41a63a65a53a67a66 a53 a45 is a probability, a82a11a78 a38a83a44a59a58a60a56 a61 a51a64a63a65a53a67a66 a53 a58a60a56 a51 a45 is a feature function (often binary) and a80 a78 is the weight ofa82a21a78 . Given a set of features and a corpus of training data, there exist ef cient training algorithms (Darroch and Ratcliff, 1972; Berger et al. , 1996) to nd the optimal parameters a89 a80 a78a14a90 . The art of building a maximum entropy parser then reduces to choosing good features.
N07-1046	J96-1002	o	This sequential property is well suited to HMMs (Vogel et al. , 1996), in which the jumps from the current aligned position can only be forward.
N07-1025	P95-1026	o	This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al. , 1998; Mihalcea and Moldovan, 1999; Agirre and Martinez, 2004), automatically bootstrapped disambiguation patterns (Yarowsky, 1995; Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab and Resnik, 2002; Ng et al. , 2003; Diab, 2004), and the use of volunteer contributions over the Web (Chklovski and Mihalcea, 2002).
W09-2501	W02-1001	o	4An adaptation of the averaged perceptron algorithm (Collins, 2002) is used to tune the model parameters.
W06-1608	P02-1040	o	3.2 Translation quality Table 2 presents the impact of parse quality on a treelet translation system, measured using BLEU (Papineni et al. , 2002).
W06-3103	P02-1040	o	5.2 Evaluation Metrics The commonly used criteria to evaluate the translation results in the machine translation community are: WER (word error rate), PER (positionindependent word error rate), BLEU (Papineni et al. , 2002), and NIST (Doddington, 2002).
W05-1512	P97-1003	o	(2005) 86.6 86.7 1.19 Klein and Manning (2003) 86.9 85.7 86.3 30.9 1.10 Charniak (1997) 87.4 87.5 1.00 Collins (1997) 88.6 88.1 0.91 Table 3: Comparison with other parsers (sentences of length  40) as head information.
J07-3002	P03-1021	o	The training data for the French/English data set is taken from the LDC Canadian Hansard data set, from which the word aligned data (presented in Och and Ney 2003) was also taken.
W07-0701	P03-1021	o	The comparison phrasal system was constructed using the same GIZA++ alignments and the heuristic combination described in (Och & Ney, 2003).
P03-1030	J93-1003	o	Second, the significance of the K-S distance in case of the null hypothesis (data sets are drawn from same distribution) can be calculated (Press et al. , 1993).
H05-1098	W96-0213	o	We used MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN.
N09-1065	P08-1002	o	(2008)], and distributional methods [e.g., Bergsma et al.
D08-1093	J93-2004	o	The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ (Marcus et al., 1993), and assume that the accuracy measure will carry over to the domains of interest.
N04-4034	J92-4003	o	Table 2: Three types of class-based MSLMs on Switchboard-I (swbd) and ICSI Meeting (mr) corpora # of swbd mr classes BROWN MMI MCMI BROWN MMI MCMI 100 68.9 0.3 68.4 0.3 68.2 0.3 78.9 3.0 77.3 2.8 76.8 2.8 500 68.9 0.3 68.3 0.3 67.9 0.3 78.7 3.1 77.1 2.8 76.7 2.8 1000 68.9 0.3 68.2 0.3 67.9 0.3 79.0 3.1 77.2 2.7 76.9 2.8 1500 69.0 0.3 68.2 0.3 68.0 0.3 79.6 3.1 77.4 2.7 77.4 2.7 2000 69.0 0.3 68.3 0.3 68.0 0.3 80.1 3.1 77.6 2.7 77.9 2.7 jV j 68.5 0.3 78.3 2.7 Table 3: Class-based MSLM on Switchboard Eval-2003 size 100 500 1000 1500 2000 jV j 3-gram 4-gram ppl 65.8 65.5 65.6 65.7 66.1 67.9 72.1 76.3 % reduction 8.6 8.9 8.8 8.7 8.3 5.8 0 -5.8 Class-based language models (Brown et al. , 1992; Whittaker and Woodland, 2003) yield great bene ts when data sparseness abounds.
W99-0622	P97-1003	o	Also, adding a constituent size/distance effect, as described by Schubert (1986) and as used by some researchers in parsing (e.g. Lesmo and Torasso (1985) and Collins (1997)) would almost certainly improve parsing.
P98-2162	J93-2003	o	The simple model 1 (Brown et al. , 1993) for the translation of a SL sentence d = dldt in a TL sentence e = el em assumes that every TL word is generated independently as a mixture of the SL words: m l P(e\[d),,~ H ~ t(ej\[di) (2) j=l i=O In the equation above t(ej\[di) stands for the probability that ej is generated by di.
D08-1004	P04-1035	o	We collect substring rationales for a sentiment classification task (Pang and Lee, 2004) and use them to obtain significant accuracy improvements for each annotator.
D08-1089	J97-3002	o	To be able identify that adjacent blocks (e.g., the development and and progress) can be merged into larger blocks, our model infers binary (non-linguistic) trees reminiscent of (Wu, 1997; Chiang, 2005).
D08-1039	D07-1007	p	Recently, word-sense disambiguation (WSD) methods have been shown to improve translation quality (Chan et al., 2007; Carpuat and Wu, 2007).
W03-2907	J92-4003	o	Since there is no practical way of determining the classification a0 which maximizes this quantity for a given corpus, (Brown et al., 1992) use a greedy algorithm which proceeds from the initial classification, performing the merge which results in the least loss in mutual information at each stage.
P98-2201	J93-2004	o	21418 examples of structures of the kind 'VB N1 PREP N2' were extracted from the Penn-TreeBank Wall Street Journal (Marcus et al. 1993).
P06-1096	N03-1017	o	The discrepancy between DEV performance and TEST performance is due to temporal distance from TRAIN and high variance in BLEU score.11 We also compared our model with Pharaoh (Koehn et al. , 2003).
W08-0307	J93-2003	o	The simple idea that words in a source chunk are typically aligned to words in a single possible target chunk is used to discard alignments which link words from 2We use IBM-1 to IBM-5 models (Brown et al., 1993) implemented with GIZA++ (Och and Ney, 2003).
W08-0302	N03-1017	o	Baseline We use the Moses MT system (Koehn et al., 2007) as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the grow-diag-finaland heuristic for symmetrization and use a maximum phrase length of 7.
W08-0409	P03-1021	o	Since manual word alignment is an ambiguous task, we also explicitly allow for ambiguous alignments, i.e. the links are marked as sure (S) or possible (P) (Och and Ney, 2003).
W01-1410	J93-2003	o	Moreover, it was (without imposing determinism) the inference technique employed in (Vidal et al. , 1993).
P08-1047	D07-1073	o	First, the Wikipedia gazetteer improved the accuracy as expected, i.e., it reproduced the result of Kazama and Torisawa (2007) for Japanese NER.
W08-0903	P02-1040	o	Techniques that analyze n-gram precision such as BLEU score (Papineni et al., 2002) have been developed with the goal of comparing candidate translations against references provided by human experts in order to determine accuracy; although in our application the candidate translator is a student and not a machine, the principle is the same, and we wish to adapt their technique to our context.
P95-1034	J93-2003	n	This approach addresses the problematic aspects of both pure knowledge-based generation (where incomplete knowledge is inevitable) and pure statistical bag generation (Brown et al. , 1993) (where the statistical system has no linguistic guidance).
W09-0210	P06-1124	o	Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results.
P06-2103	P03-1021	o	In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical (Lapata, 2003) and global levels (Barzilay and Lee, 2004), while preserving regularities recognized by classic discourse theories (Barzilay and Lapata, 2005).
P09-2044	J96-2004	o	We then used Cohens Kappa () to determine the level of agreement (Carletta, 1996).
P06-1001	P03-1021	o	Decoding weights are optimized using Ochs algorithm (Och, 2003) to set weights for the four components of the loglinear model: language model, phrase translation model, distortion model, and word-length feature.
P03-1043	P95-1026	o	The tag propagation/elimination scheme is adopted from (Yarowsky 1995).
N06-1008	N03-1003	o	Previous attempts have used, for instance, the similarities between case frames (Lin and Pan57 tel, 2001), anchor words (Barzilay and Lee, 2003; Shinyama et al. , 2002; Szepektor et al. , 2004), and a web-based method(Szepektor et al. , 2004;Geffet and Dagan, 2005).
P98-1037	P95-1026	o	The adaptive approach is somehow similar to their idea of incremental learning and to the bootstrap approach proposed by Yarowsky (1995).
N04-1023	P03-1021	o	By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al. , 2001) score on the test dataset was improved from 31.6% to 32.9%.
W95-0101	J93-2004	o	Below is an example of the initial-state tagging of a sentence from the Penn Treebank \[Marcus et al. , 1993\], where an underscore is to be read as or.
C08-1051	C98-2122	o	Applications of word clustering include language modeling (Brown et al., 1992), text classification (Baker and McCallum, 1998), thesaurus construction (Lin, 1998) and so on.
E09-1071	C08-1114	o	One such relational reasoning task is the problem of compound noun interpretation, which has received a great deal of attention in recent years (Girju et al., 2005; Turney, 2006; Butnariu and Veale, 2008).
P08-1064	J97-3002	o	The formally syntax-based model for SMT was first advocated by Wu (1997).
N09-1051	N04-1039	o	Goodman, 2004) and lscript22 regularization (Lau, 1994; Chen and Rosenfeld, 2000; Lebanon and Lafferty, 2001).
D07-1129	P04-1015	o	2.3 Online Learning Again following (McDonald et al. , 2005), we have used the single best MIRA (Crammer and Singer, 2003), which is a margin aware variant of perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction.
H05-1021	J93-2003	o	The 1000-best lists are augmented with IBM Model-1 (Brown et al. , 1993) scores and then rescored with a second set of MET parameters.
